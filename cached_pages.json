[
  {
    "id": "1cbf0b00-3427-806e-a133-c409031d4a94",
    "title": "USEFUL CODE SNIPPETS ",
    "content": "Code snippet to  unzip a zip file in google  colab \nimport zipfile\nimport os\n\nwith zipfile.ZipFile('/content/drive/MyDrive/fer_dataset/train.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('/content/sample_data/FER')\n\nwith zipfile.ZipFile('/content/drive/MyDrive/fer_dataset/val.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('/content/sample_data/FER')\nCode snippet to make a list of  folder names inside a folder:\nUseful for making custom dataset for dataset loader in pytorch \n# Get all folder names \nclasses = sorted([d for d in os.listdir(self.data_dir) \n                    if os.path.isdir(os.path.join(self.data_dir, d))])\nThe correct way of  running a c++ program with another version of std \n\ng++ -std=c++11 main.cpp && ./a.out\n\nqdrant calling cluster \nfrom qdrant_client import QdrantClient\n\nqdrant_client = QdrantClient(\n    url=\"https://dbc8e6e9-8d06-4c7d-9757-e82e00342e8e.us-east4-0.gcp.cloud.qdrant.io:6333\", \n    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.Ry5CoAOFS_8wl_f-ayzDWGxBIDmnaWybeNeLeXFH8I4\",\n)\n\nprint(qdrant_client.get_collections())\nTo remove\u00a0s[i]\u00a0from\u00a0str\u00a0after appending it, the\u00a0erase()\u00a0method can be used.\u00a0It removes characters from a string at a specified position.\u00a0The position to erase from would be the index immediately before the newly added character, which is\u00a0str.size() - 1\u00a0after the append operation.\n\n#include <iostream>\n#include <string>\n\nint main() {\n    std::string str = \"initial\";\n    std::string s = \"source\";\n    int i = 3;\n\n    str += s[i];\n    std::cout << \"String after append: \" << str << std::endl;\n\n    str.erase(str.size() - 1, 1);\n    std::cout << \"String after erase: \" << str << std::endl;\n\n    return 0;\n}"
  },
  {
    "id": "1cbf0b00-3427-80f1-b986-ee05e1f863fe",
    "title": "LangChain and LangGraph: A Comprehensive Guide",
    "content": "\nTable of Contents\nIntroduction to LangChain\nGetting Started with LangChain\nCore Concepts in LangChain\nBuilding Applications with LangChain\nIntroduction to LangGraph\nLangGraph Fundamentals\nAdvanced LangGraph Techniques\nIntegrating LangChain with LangGraph\nBuilding Advanced Applications\nBest Practices and Optimization\nIntroduction to LangChain\nLangChain is a framework designed to simplify the development of applications using large language models (LLMs). It provides a set of tools, components, and interfaces that make it easier to build applications that leverage the power of LLMs for various tasks.\nWhat is LangChain?\nLangChain is built on the following principles:\nComposability: LangChain makes it easy to combine LLMs with other components like memory systems, data sources, and tools.\nIntegration: It provides pre-built integrations with various LLM providers, data sources, and tools.\nUse-case optimization: It offers optimized chains for common use cases like question answering, summarization, and chat.\nKey Components\nLangChain consists of several key components:\nLanguage Models: Wrappers around LLMs from providers like OpenAI, Anthropic, Cohere, etc.\nPrompts: Templates and tools for creating and managing prompts.\nMemory: Systems for maintaining state in multi-turn interactions.\nChains: Sequences of operations that can be executed together.\nAgents: Systems that can use tools and make decisions.\nTools: Utilities that agents can use to interact with external systems.\nInstallation\nTo get started with LangChain, you'll need to install the package:\npip install langchain\n\nIf you want to use specific integrations, you'll need to install additional packages:\npip install langchain-openai  # For OpenAI integrations\npip install langchain-anthropic  # For Anthropic integrations\npip install langchain-community  # For community integrations\n\nGetting Started with LangChain\nLet's start with a simple example of using LangChain to interact with an LLM:\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import HumanMessage\nimport os\n\n# Set your API key\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\n# Initialize the LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\n# Use the LLM\nresponse = llm.invoke([HumanMessage(content=\"Tell me a joke about programming.\")])\nprint(response.content)\n\nThis example demonstrates the basic pattern of using LangChain:\nImport the necessary components\nSet up any required API keys\nInitialize the LLM\nUse the LLM to process a request\nUsing Prompts\nLangChain provides tools for creating and managing prompts:\nfrom langchain.prompts import ChatPromptTemplate\n\n# Create a prompt template\nprompt = ChatPromptTemplate.from_template(\n    \"Tell me a joke about {topic}.\"\n)\n\n# Format the prompt with specific inputs\nformatted_prompt = prompt.format_messages(topic=\"programming\")\n\n# Use the formatted prompt with the LLM\nresponse = llm.invoke(formatted_prompt)\nprint(response.content)\n\nCore Concepts in LangChain\nChains\nChains are sequences of operations involving LLMs and other components. Here's a simple example:\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\n# Create a prompt template\nprompt = ChatPromptTemplate.from_template(\n    \"Tell me a joke about {topic}.\"\n)\n\n# Create a chain\nchain = LLMChain(llm=llm, prompt=prompt)\n\n# Run the chain\nresult = chain.invoke({\"topic\": \"programming\"})\nprint(result[\"text\"])\n\nMemory\nMemory systems help maintain state in multi-turn interactions:\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\n# Create a memory system\nmemory = ConversationBufferMemory()\n\n# Create a conversation chain with memory\nconversation = ConversationChain(\n    llm=llm,\n    memory=memory,\n    verbose=True\n)\n\n# Start the conversation\nresponse1 = conversation.invoke(input=\"Hi, my name is Alice.\")\nprint(response1[\"response\"])\n\n# Continue the conversation\nresponse2 = conversation.invoke(input=\"What's my name?\")\nprint(response2[\"response\"])\n\nAgents\nAgents use LLMs to decide what actions to take:\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain.tools import DuckDuckGoSearchRun\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n\n# Create tools for the agent to use\nsearch = DuckDuckGoSearchRun()\n\n# Create the agent\nagent = initialize_agent(\n    tools=[search],\n    llm=llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n\n# Run the agent\nresponse = agent.invoke(input=\"What's the capital of France?\")\nprint(response[\"output\"])\n\nTools\nTools allow LLMs to interact with external systems:\nfrom langchain.tools import DuckDuckGoSearchRun, BaseTool\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain_openai import ChatOpenAI\nfrom typing import Any, Dict, Optional, Type\nfrom pydantic import BaseModel, Field\n\n# Define a custom tool\nclass CalculatorInput(BaseModel):\n    \"\"\"Input for the calculator tool.\"\"\"\n    operation: str = Field(description=\"The operation to perform (add, subtract, multiply, divide).\")\n    a: float = Field(description=\"The first number.\")\n    b: float = Field(description=\"The second number.\")\n\nclass CalculatorTool(BaseTool):\n    name = \"calculator\"\n    description = \"Useful for performing mathematical calculations.\"\n    args_schema: Type[BaseModel] = CalculatorInput\n\n    def _run(self, operation: str, a: float, b: float, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Run the tool.\"\"\"\n        if operation == \"add\":\n            return {\"result\": a + b}\n        elif operation == \"subtract\":\n            return {\"result\": a - b}\n        elif operation == \"multiply\":\n            return {\"result\": a * b}\n        elif operation == \"divide\":\n            if b == 0:\n                return {\"error\": \"Cannot divide by zero.\"}\n            return {\"result\": a / b}\n        else:\n            return {\"error\": f\"Unknown operation: {operation}\"}\n\n# Initialize the LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n\n# Create tools\nsearch = DuckDuckGoSearchRun()\ncalculator = CalculatorTool()\n\n# Create the agent\nagent = initialize_agent(\n    tools=[search, calculator],\n    llm=llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n\n# Run the agent\nresponse = agent.invoke(input=\"What's 25 * 12?\")\nprint(response[\"output\"])\n\nBuilding Applications with LangChain\nNow let's explore how to build more complex applications using LangChain.\nQuestion Answering over Documents\nfrom langchain.chains import RetrievalQA\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain.document_loaders import TextLoader\n\n# Load the document\nloader = TextLoader(\"path/to/your/document.txt\")\ndocuments = loader.load()\n\n# Split the documents into chunks\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)\n\n# Create a vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = FAISS.from_documents(texts, embeddings)\n\n# Create a retriever\nretriever = vectorstore.as_retriever()\n\n# Initialize the LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\n# Create a QA chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=retriever,\n    verbose=True\n)\n\n# Ask a question\nquery = \"What is the main topic of this document?\"\nresponse = qa_chain.invoke({\"query\": query})\nprint(response[\"result\"])\n\nChat Applications\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain.document_loaders import TextLoader\n\n# Load the document\nloader = TextLoader(\"path/to/your/document.txt\")\ndocuments = loader.load()\n\n# Split the documents into chunks\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)\n\n# Create a vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = FAISS.from_documents(texts, embeddings)\n\n# Create a retriever\nretriever = vectorstore.as_retriever()\n\n# Initialize the LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\n# Create a memory system\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\",\n    return_messages=True\n)\n\n# Create a conversational QA chain\nconversation_chain = ConversationalRetrievalChain.from_llm(\n    llm=llm,\n    retriever=retriever,\n    memory=memory\n)\n\n# Start the conversation\nresponse = conversation_chain.invoke({\"question\": \"What is the main topic of this document?\"})\nprint(response[\"answer\"])\n\n# Continue the conversation\nresponse = conversation_chain.invoke({\"question\": \"Can you tell me more about it?\"})\nprint(response[\"answer\"])\n\nStructured Output\nfrom langchain.pydantic_v1 import BaseModel, Field\nfrom langchain.output_parsers import PydanticOutputParser\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom typing import List\n\n# Define the output schema\nclass Movie(BaseModel):\n    title: str = Field(description=\"Title of the movie\")\n    director: str = Field(description=\"Director of the movie\")\n    year: int = Field(description=\"Year the movie was released\")\n    genres: List[str] = Field(description=\"Genres of the movie\")\n\n# Create an output parser\nparser = PydanticOutputParser(pydantic_object=Movie)\n\n# Create a prompt template\nprompt = PromptTemplate(\n    template=\"Extract the movie information from the following text:\\n{text}\\n{format_instructions}\",\n    input_variables=[\"text\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n)\n\n# Initialize the LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\n# Create the chain\nchain = prompt | llm | parser\n\n# Extract movie information\ntext = \"Inception is a 2010 science fiction action film written and directed by Christopher Nolan. The film stars Leonardo DiCaprio as a professional thief who steals information by infiltrating the subconscious of his targets.\"\nmovie = chain.invoke({\"text\": text})\nprint(movie)\n\nIntroduction to LangGraph\nLangGraph is an extension of LangChain that allows you to build stateful, multi-actor applications with language models. It's designed to help you create complex applications that involve multiple steps, feedback loops, and interactions between different components.\nWhat is LangGraph?\nLangGraph is a library for building stateful, multi-actor applications using language models. It's built on top of LangChain and provides a way to define the flow of operations in your application using a graph-based approach.\nKey Concepts in LangGraph\nNodes: Represent operations or steps in your application.\nEdges: Define the flow between nodes.\nState: Represents the current state of your application.\nAgents: Actors that can make decisions and take actions.\nInstallation\nTo get started with LangGraph, you'll need to install the package:\npip install langgraph\n\nLangGraph Fundamentals\nLet's start with a simple example of using LangGraph to create a graph-based application:\nfrom langgraph.graph import StateGraph, END\nfrom typing import Dict, TypedDict, Annotated, Sequence\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\n# Define the state\nclass GraphState(TypedDict):\n    input: str\n    result: str\n\n# Define the processing step\ndef process(state: GraphState) -> GraphState:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"Explain {state['input']} in simple terms.\")])\n    return {\"result\": response.content}\n\n# Create the graph\ngraph = StateGraph(GraphState)\n\n# Add nodes to the graph\ngraph.add_node(\"process\", process)\n\n# Define the flow\ngraph.set_entry_point(\"process\")\ngraph.add_edge(\"process\", END)\n\n# Compile the graph\ncompiled_graph = graph.compile()\n\n# Run the graph\nresponse = compiled_graph.invoke({\"input\": \"quantum computing\"})\nprint(response[\"result\"])\n\nConditional Flows\nLet's create a more complex graph with conditional flows:\nfrom langgraph.graph import StateGraph, END\nfrom typing import Dict, TypedDict, Annotated, Sequence, Literal\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\n# Define the state\nclass GraphState(TypedDict):\n    input: str\n    result: str\n    topic: str\n    complexity: Literal[\"simple\", \"detailed\"]\n\n# Define the categorization step\ndef categorize(state: GraphState) -> GraphState:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"Categorize this query: {state['input']}. Respond with only one word representing the topic.\")])\n    return {\"topic\": response.content.strip()}\n\n# Define the complexity assessment step\ndef assess_complexity(state: GraphState) -> Literal[\"simple\", \"detailed\"]:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"Is this query asking for a simple or detailed explanation? Query: {state['input']}. Respond with only 'simple' or 'detailed'.\")])\n    return response.content.strip().lower()\n\n# Define the simple explanation step\ndef generate_simple_explanation(state: GraphState) -> GraphState:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"Explain {state['input']} in simple terms, suitable for a beginner.\")])\n    return {\"result\": response.content}\n\n# Define the detailed explanation step\ndef generate_detailed_explanation(state: GraphState) -> GraphState:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"Provide a detailed explanation of {state['input']}, including technical details and examples.\")])\n    return {\"result\": response.content}\n\n# Create the graph\ngraph = StateGraph(GraphState)\n\n# Add nodes to the graph\ngraph.add_node(\"categorize\", categorize)\ngraph.add_node(\"assess_complexity\", assess_complexity)\ngraph.add_node(\"simple_explanation\", generate_simple_explanation)\ngraph.add_node(\"detailed_explanation\", generate_detailed_explanation)\n\n# Define the flow\ngraph.set_entry_point(\"categorize\")\ngraph.add_edge(\"categorize\", \"assess_complexity\")\ngraph.add_conditional_edges(\n    \"assess_complexity\",\n    {\n        \"simple\": \"simple_explanation\",\n        \"detailed\": \"detailed_explanation\"\n    }\n)\ngraph.add_edge(\"simple_explanation\", END)\ngraph.add_edge(\"detailed_explanation\", END)\n\n# Compile the graph\ncompiled_graph = graph.compile()\n\n# Run the graph\nresponse = compiled_graph.invoke({\"input\": \"quantum computing\"})\nprint(response[\"result\"])\n\nFeedback Loops\nLangGraph also supports feedback loops, which allow you to iterate on a solution until it meets certain criteria:\nfrom langgraph.graph import StateGraph, END\nfrom typing import Dict, TypedDict, Annotated, Sequence, Literal, Union\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import HumanMessage\nimport re\n\n# Define the state\nclass GraphState(TypedDict):\n    input: str\n    draft: str\n    final_answer: str\n\n# Define the drafting step\ndef draft_answer(state: GraphState) -> GraphState:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"Draft an answer to the following question: {state['input']}\")])\n    return {\"draft\": response.content}\n\n# Define the review step\ndef review_draft(state: GraphState) -> Literal[\"approve\", \"revise\"]:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"Review this draft answer to the question '{state['input']}'. The draft is: {state['draft']}. Does this answer completely and accurately address the question? Respond with only 'approve' or 'revise'.\")])\n    return response.content.strip().lower()\n\n# Define the revision step\ndef revise_draft(state: GraphState) -> GraphState:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"Revise this draft answer to the question '{state['input']}'. The current draft is: {state['draft']}. Improve it to make it more complete and accurate.\")])\n    return {\"draft\": response.content}\n\n# Define the finalization step\ndef finalize_answer(state: GraphState) -> GraphState:\n    return {\"final_answer\": state[\"draft\"]}\n\n# Create the graph\ngraph = StateGraph(GraphState)\n\n# Add nodes to the graph\ngraph.add_node(\"draft\", draft_answer)\ngraph.add_node(\"review\", review_draft)\ngraph.add_node(\"revise\", revise_draft)\ngraph.add_node(\"finalize\", finalize_answer)\n\n# Define the flow\ngraph.set_entry_point(\"draft\")\ngraph.add_edge(\"draft\", \"review\")\ngraph.add_conditional_edges(\n    \"review\",\n    {\n        \"approve\": \"finalize\",\n        \"revise\": \"revise\"\n    }\n)\ngraph.add_edge(\"revise\", \"review\")\ngraph.add_edge(\"finalize\", END)\n\n# Compile the graph\ncompiled_graph = graph.compile()\n\n# Run the graph\nresponse = compiled_graph.invoke({\"input\": \"Explain the theory of relativity\"})\nprint(response[\"final_answer\"])\n\nAdvanced LangGraph Techniques\nParallel Processing\nLangGraph allows you to process multiple nodes in parallel:\nfrom langgraph.graph import StateGraph, END\nfrom typing import Dict, TypedDict, Annotated, Sequence, Literal, List\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\n# Define the state\nclass GraphState(TypedDict):\n    input: str\n    scientific_explanation: str\n    historical_context: str\n    practical_applications: str\n    combined_result: str\n\n# Define the scientific explanation step\ndef generate_scientific_explanation(state: GraphState) -> GraphState:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"Provide a scientific explanation of {state['input']}.\")])\n    return {\"scientific_explanation\": response.content}\n\n# Define the historical context step\ndef generate_historical_context(state: GraphState) -> GraphState:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"Provide the historical context of {state['input']}.\")])\n    return {\"historical_context\": response.content}\n\n# Define the practical applications step\ndef generate_practical_applications(state: GraphState) -> GraphState:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"List the practical applications of {state['input']}.\")])\n    return {\"practical_applications\": response.content}\n\n# Define the combination step\ndef combine_results(state: GraphState) -> GraphState:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    prompt = f\"\"\"\n    Create a comprehensive overview of {state['input']} by combining the following sections:\n\n    Scientific Explanation:\n    {state['scientific_explanation']}\n\n    Historical Context:\n    {state['historical_context']}\n\n    Practical Applications:\n    {state['practical_applications']}\n    \"\"\"\n    response = llm.invoke([HumanMessage(content=prompt)])\n    return {\"combined_result\": response.content}\n\n# Create the graph\ngraph = StateGraph(GraphState)\n\n# Add nodes to the graph\ngraph.add_node(\"scientific_explanation\", generate_scientific_explanation)\ngraph.add_node(\"historical_context\", generate_historical_context)\ngraph.add_node(\"practical_applications\", generate_practical_applications)\ngraph.add_node(\"combine\", combine_results)\n\n# Define the flow\ngraph.set_entry_point(\"scientific_explanation\")\ngraph.add_edge(\"scientific_explanation\", \"historical_context\")\ngraph.add_edge(\"historical_context\", \"practical_applications\")\ngraph.add_edge(\"practical_applications\", \"combine\")\ngraph.add_edge(\"combine\", END)\n\n# Compile the graph\ncompiled_graph = graph.compile()\n\n# Run the graph\nresponse = compiled_graph.invoke({\"input\": \"quantum computing\"})\nprint(response[\"combined_result\"])\n\nMulti-Agent Systems\nLangGraph can be used to create multi-agent systems where different agents collaborate to solve a problem:\nfrom langgraph.graph import StateGraph, END\nfrom typing import Dict, TypedDict, Annotated, Sequence, Literal, List, Union\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import HumanMessage\nimport json\n\n# Define the state\nclass GraphState(TypedDict):\n    input: str\n    research_results: str\n    analysis: str\n    critique: str\n    final_report: str\n    current_agent: str\n\n# Define the researcher agent\ndef researcher(state: GraphState) -> GraphState:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"You are a research assistant. Find key information about: {state['input']}\")])\n    return {\"research_results\": response.content, \"current_agent\": \"analyst\"}\n\n# Define the analyst agent\ndef analyst(state: GraphState) -> GraphState:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"You are a data analyst. Analyze this research: {state['research_results']}\")])\n    return {\"analysis\": response.content, \"current_agent\": \"critic\"}\n\n# Define the critic agent\ndef critic(state: GraphState) -> GraphState:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"You are a critic. Provide constructive criticism on this analysis: {state['analysis']}\")])\n    return {\"critique\": response.content, \"current_agent\": \"reporter\"}\n\n# Define the reporter agent\ndef reporter(state: GraphState) -> GraphState:\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    prompt = f\"\"\"\n    Create a comprehensive report on {state['input']} based on the following:\n\n    Research:\n    {state['research_results']}\n\n    Analysis:\n    {state['analysis']}\n\n    Critique:\n    {state['critique']}\n    \"\"\"\n    response = llm.invoke([HumanMessage(content=prompt)])\n    return {\"final_report\": response.content, \"current_agent\": \"done\"}\n\n# Define the router\ndef route(state: GraphState) -> Literal[\"researcher\", \"analyst\", \"critic\", \"reporter\", \"end\"]:\n    current_agent = state.get(\"current_agent\", \"researcher\")\n    if current_agent == \"done\":\n        return \"end\"\n    return current_agent\n\n# Create the graph\ngraph = StateGraph(GraphState)\n\n# Add nodes to the graph\ngraph.add_node(\"researcher\", researcher)\ngraph.add_node(\"analyst\", analyst)\ngraph.add_node(\"critic\", critic)\ngraph.add_node(\"reporter\", reporter)\n\n# Define the flow\ngraph.set_entry_point(\"researcher\")\ngraph.add_conditional_edges(\n    \"researcher\",\n    lambda x: route(x)\n)\ngraph.add_conditional_edges(\n    \"analyst\",\n    lambda x: route(x)\n)\ngraph.add_conditional_edges(\n    \"critic\",\n    lambda x: route(x)\n)\ngraph.add_conditional_edges(\n    \"reporter\",\n    lambda x: route(x)\n)\ngraph.add_edge(\"end\", END)\n\n# Compile the graph\ncompiled_graph = graph.compile()\n\n# Run the graph\nresponse = compiled_graph.invoke({\"input\": \"the impact of artificial intelligence on society\"})\nprint(response[\"final_report\"])\n\nAdvanced State Management\nLangGraph provides advanced state management capabilities:\nfrom langgraph.graph import StateGraph, END\nfrom typing import Dict, TypedDict, Annotated, Sequence, Literal, List, Optional\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import HumanMessage\nimport json\n\n# Define the state\nclass ChatMessage(TypedDict):\n    role: str\n    content: str\n\nclass GraphState(TypedDict):\n    messages: List[ChatMessage]\n    current_thought: Optional[str]\n    search_results: Optional[str]\n    response: Optional[str]\n\n# Define the thinking step\ndef think(state: GraphState) -> GraphState:\n    messages = state[\"messages\"]\n    last_message = messages[-1][\"content\"]\n\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"You are an AI assistant. The user said: '{last_message}'. Think about how to respond to this. Write your thoughts.\")])\n\n    return {\"current_thought\": response.content}\n\n# Define the search step\ndef search(state: GraphState) -> GraphState:\n    messages = state[\"messages\"]\n    last_message = messages[-1][\"content\"]\n\n    # Simulate a search operation\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke([HumanMessage(content=f\"Simulate search results for: {last_message}\")])\n\n    return {\"search_results\": response.content}\n\n# Define the response generation step\ndef generate_response(state: GraphState) -> GraphState:\n    messages = state[\"messages\"]\n    last_message = messages[-1][\"content\"]\n    current_thought = state.get(\"current_thought\", \"\")\n    search_results = state.get(\"search_results\", \"\")\n\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    prompt = f\"\"\"\n    Generate a response to the user's message:\n\n    User's message: {last_message}\n\n    Your thoughts: {current_thought}\n\n    Search results: {search_results}\n    \"\"\"\n    response = llm.invoke([HumanMessage(content=prompt)])\n\n    return {\"response\": response.content}\n\n# Define the router\ndef should_search(state: GraphState) -> Literal[\"search\", \"no_search\"]:\n    messages = state[\"messages\"]\n    last_message = messages[-1][\"content\"]\n\n    # Check if the message likely requires search\n    keywords = [\"search\", \"find\", \"information\", \"data\", \"lookup\", \"what is\", \"how to\"]\n    if any(keyword in last_message.lower() for keyword in keywords):\n        return \"search\"\n    return \"no_search\"\n\n# Create the graph\ngraph = StateGraph(GraphState)\n\n# Add nodes to the graph\ngraph.add_node(\"think\", think)\ngraph.add_node(\"search\", search)\ngraph.add_node(\"generate_response\", generate_response)\n\n# Define the flow\ngraph.set_entry_point(\"think\")\ngraph.add_conditional_edges(\n    \"think\",\n    lambda x: should_search(x),\n    {\n        \"search\": \"search\",\n        \"no_search\": \"generate_response\"\n    }\n)\ngraph.add_edge(\"search\", \"generate_response\")\ngraph.add_edge(\"generate_response\", END)\n\n# Compile the graph\ncompiled_graph = graph.compile()\n\n# Run the graph\nresponse = compiled_graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]})\nprint(response[\"response\"])\n\nIntegrating LangChain with LangGraph\nLangChain and LangGraph are complementary frameworks that work exceptionally well together. LangChain provides the building blocks (chains, retrievers, memory, tools), while LangGraph gives you the structure to orchestrate these components into sophisticated workflows.\nCompleting Your Integration Example\nLet's expand the code you've started to create a complete RAG (Retrieval-Augmented Generation) system that:\nTakes a user query\nRetrieves relevant documents\nGenerates an answer based on those documents\nOptionally refines the answer if needed\n\nAdvanced Langchain + Langgraph RAG   patterns \nfrom langgraph.graph import StateGraph, END\nfrom typing import Dict, TypedDict, Annotated, Sequence, Literal, List, Union\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.retrievers import BM25Retriever\nfrom langchain.schema.document import Document\nfrom langchain.schema.runnable import RunnablePassthrough, RunnableLambda\nimport os\n\n# Define the state structure\nclass GraphState(TypedDict):\n    query: str\n    retrieved_documents: List[Document]\n    answer: str\n    needs_refinement: bool\n\n# Document retrieval function\ndef retrieve_documents(state: GraphState) -> Dict:\n    # Sample documents - in a real app, these would come from a vector store\n    documents = [\n        Document(page_content=\"Paris is the capital of France and is known for the Eiffel Tower.\"),\n        Document(page_content=\"London is the capital of the UK and home to the British monarchy.\"),\n        Document(page_content=\"Berlin is the capital of Germany and has a rich history.\"),\n        Document(page_content=\"Tokyo is the capital of Japan and the most populous metropolitan area in the world.\"),\n        Document(page_content=\"Beijing is the capital of China and contains the Forbidden City.\"),\n    ]\n\n    # Create a retriever\n    retriever = BM25Retriever.from_documents(documents)\n\n    # Retrieve relevant documents\n    retrieved_docs = retriever.get_relevant_documents(state[\"query\"])\n    \n    return {\"retrieved_documents\": retrieved_docs}\n\n# Answer generation function using LangChain's components\ndef generate_answer(state: GraphState) -> Dict:\n    llm = ChatOpenAI(temperature=0)\n    \n    # Create a prompt template for RAG\n    prompt = ChatPromptTemplate.from_messages([\n        SystemMessage(content=\"You are a helpful assistant. Answer the question based on the provided context.\"),\n        (\"human\", \"Context:\\n{context}\\n\\nQuestion: {question}\")\n    ])\n    \n    # Format docs into a string\n    def format_docs(docs):\n        return \"\\n\\n\".join(doc.page_content for doc in docs)\n    \n    # Create a chain\n    rag_chain = (\n        {\"context\": lambda x: format_docs(x[\"retrieved_documents\"]), \n         \"question\": lambda x: x[\"query\"]}\n        | prompt\n        | llm\n        | (lambda x: {\"answer\": x.content})\n    )\n    \n    # Run the chain\n    result = rag_chain.invoke(state)\n    \n    # Check if answer needs refinement\n    needs_refinement = len(state[\"retrieved_documents\"]) == 0 or len(result[\"answer\"]) < 20\n    result[\"needs_refinement\"] = needs_refinement\n    \n    return result\n\n# Refine answer if needed\ndef refine_answer(state: GraphState) -> Dict:\n    llm = ChatOpenAI(temperature=0.2)  # Slightly more creative for refinement\n    \n    prompt = ChatPromptTemplate.from_messages([\n        SystemMessage(content=\"You are a helpful assistant. The previous answer may be incomplete. Please improve it.\"),\n        (\"human\", \"Original query: {query}\\nPrevious answer: {answer}\\n\\nPlease provide a more comprehensive answer.\")\n    ])\n    \n    refine_chain = prompt | llm | (lambda x: {\"answer\": x.content, \"needs_refinement\": False})\n    \n    return refine_chain.invoke(state)\n\n# Decision function\ndef should_refine(state: GraphState) -> Literal[\"refine\", \"end\"]:\n    if state[\"needs_refinement\"]:\n        return \"refine\"\n    else:\n        return \"end\"\n\n# Create the graph\ndef create_rag_graph():\n    # Initialize the graph\n    graph = StateGraph(GraphState)\n    \n    # Add nodes\n    graph.add_node(\"retrieve\", retrieve_documents)\n    graph.add_node(\"generate\", generate_answer)\n    graph.add_node(\"refine\", refine_answer)\n    \n    # Build the graph structure\n    graph.add_edge(\"retrieve\", \"generate\")\n    graph.add_conditional_edges(\n        \"generate\",\n        should_refine,\n        {\n            \"refine\": \"refine\",\n            \"end\": END\n        }\n    )\n    graph.add_edge(\"refine\", END)\n    \n    # Set the entry point\n    graph.set_entry_point(\"retrieve\")\n    \n    # Compile the graph\n    return graph.compile()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create the graph\n    rag_graph = create_rag_graph()\n    \n    # Run the graph with a query\n    result = rag_graph.invoke({\"query\": \"What is the capital of France?\"})\n    \n    print(\"Query:\", result[\"query\"])\n    print(\"\\nRetrieved Documents:\")\n    for doc in result[\"retrieved_documents\"]:\n        print(f\"- {doc.page_content}\")\n    print(\"\\nFinal Answer:\", result[\"answer\"])\n\nAdvanced Langchain + Langgraph  integration patterns \n\nfrom langgraph.graph import StateGraph, END\nfrom typing import Dict, TypedDict, List, Tuple, Annotated, Literal, Optional\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.output_parsers import PydanticOutputParser\nfrom langchain.tools import Tool\nfrom langchain.agents import AgentExecutor, create_react_agent\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom pydantic import BaseModel, Field\nimport json\n\n# Pattern 1: Agent-based LangGraph Node\n# =====================================\n\n# Define a state that includes chat history\nclass AgentGraphState(TypedDict):\n    messages: List[Union[HumanMessage, AIMessage]]\n    agent_outcome: Optional[str]\n    final_response: Optional[str]\n\n# Create a simple calculator tool\ndef calculator(expression: str) -> float:\n    \"\"\"Evaluate a mathematical expression.\"\"\"\n    try:\n        return eval(expression)\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Create LangChain tools\ntools = [\n    Tool.from_function(\n        func=calculator,\n        name=\"Calculator\",\n        description=\"Useful for performing math calculations\"\n    )\n]\n\n# Create an agent using LangChain's components\ndef create_langchain_agent():\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n    \n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"You are a helpful assistant with access to tools.\"),\n        MessagesPlaceholder(variable_name=\"messages\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n    ])\n    \n    agent = create_react_agent(llm, tools, prompt)\n    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n    \n    return agent_executor\n\n# LangGraph node that uses a LangChain agent\ndef run_agent(state: AgentGraphState) -> Dict:\n    # Extract the last human message\n    last_human = next((msg for msg in reversed(state[\"messages\"]) \n                      if isinstance(msg, HumanMessage)), None)\n    \n    if not last_human:\n        return {\"agent_outcome\": \"No human message found\"}\n    \n    # Run the agent\n    agent_executor = create_langchain_agent()\n    result = agent_executor.invoke({\"input\": last_human.content})\n    \n    return {\"agent_outcome\": result[\"output\"]}\n\n# Format final response\ndef format_response(state: AgentGraphState) -> Dict:\n    return {\"final_response\": f\"Agent result: {state['agent_outcome']}\"}\n\n# Pattern 2: Streaming with LangChain and LangGraph\n# ================================================\n\n# Define a streaming version of our RAG architecture\ndef create_streaming_rag_graph():\n    from langchain_core.runnables import RunnableConfig\n    \n    # Similar setup as before but with streaming\n    class StreamingState(TypedDict):\n        query: str\n        context: str\n        response: str\n    \n    def retrieve(state: StreamingState) -> Dict:\n        # Simulating document retrieval\n        if \"France\" in state[\"query\"]:\n            return {\"context\": \"Paris is the capital of France. It's known for the Eiffel Tower.\"}\n        else:\n            return {\"context\": \"No relevant information found.\"}\n    \n    def generate(state: StreamingState) -> Dict:\n        llm = ChatOpenAI(temperature=0, streaming=True)\n        prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"Answer based on context: {context}\"),\n            (\"human\", \"{query}\")\n        ])\n        \n        chain = prompt | llm\n        \n        # Return a generator that will stream tokens\n        def stream_response():\n            for chunk in chain.stream({\n                \"context\": state[\"context\"],\n                \"query\": state[\"query\"]\n            }):\n                yield {\"response\": chunk.content}\n        \n        return stream_response()\n    \n    # Create a graph with streaming\n    graph = StateGraph(StreamingState)\n    graph.add_node(\"retrieve\", retrieve)\n    graph.add_node(\"generate\", generate)\n    graph.add_edge(\"retrieve\", \"generate\")\n    graph.add_edge(\"generate\", END)\n    graph.set_entry_point(\"retrieve\")\n    \n    return graph.compile()\n\n# Pattern 3: Tool Calling with LangChain and LangGraph\n# ===================================================\n\n# Define structured output schema for tool calling\nclass SearchQuery(BaseModel):\n    query: str = Field(description=\"The search query\")\n    filters: Optional[Dict] = Field(description=\"Optional search filters\", default=None)\n\nclass ResearchAction(BaseModel):\n    action: Literal[\"search\", \"summarize\", \"extract_data\"] = Field(description=\"The type of research action to perform\")\n    search_query: Optional[SearchQuery] = Field(description=\"Search parameters if action is 'search'\", default=None)\n    content_to_process: Optional[str] = Field(description=\"Content to process for summarize or extract_data actions\", default=None)\n\n# Research workflow with tool calling\nclass ResearchState(TypedDict):\n    query: str\n    actions: List[ResearchAction]\n    search_results: Optional[List[str]]\n    final_result: Optional[str]\n\ndef parse_research_actions(state: ResearchState) -> Dict:\n    llm = ChatOpenAI(temperature=0)\n    \n    # Use Pydantic parser with LangChain\n    parser = PydanticOutputParser(pydantic_object=ResearchAction)\n    \n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"\"\"You are a research assistant. Based on the user's query, decide on the next research action to take.\n         {format_instructions}\"\"\"),\n        (\"human\", \"{query}\")\n    ])\n    \n    chain = prompt.format_prompt(\n        query=state[\"query\"],\n        format_instructions=parser.get_format_instructions()\n    ) | llm | parser\n    \n    # Get the next action\n    action = chain.invoke({})\n    \n    return {\"actions\": [action]}\n\ndef execute_search(state: ResearchState) -> Dict:\n    # Simple mock search function\n    actions = state[\"actions\"]\n    last_action = actions[-1] if actions else None\n    \n    if last_action and last_action.action == \"search\" and last_action.search_query:\n        # Mock search results\n        results = [\n            f\"Result 1 for {last_action.search_query.query}\",\n            f\"Result 2 for {last_action.search_query.query}\",\n            f\"Result 3 for {last_action.search_query.query}\"\n        ]\n        return {\"search_results\": results}\n    \n    return {\"search_results\": []}\n\ndef generate_research_summary(state: ResearchState) -> Dict:\n    llm = ChatOpenAI(temperature=0)\n    \n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"Summarize the research results.\"),\n        (\"human\", \"Query: {query}\\nSearch Results: {search_results}\")\n    ])\n    \n    chain = prompt | llm | (lambda x: {\"final_result\": x.content})\n    \n    return chain.invoke(state)\n\n# Pattern 4: Hybrid Chain/Graph Using LCEL\n# =======================================\n\ndef create_hybrid_chain_graph():\n    from langchain_core.runnables import RunnablePassthrough\n    \n    # Create a LangChain chain using LCEL\n    llm = ChatOpenAI(temperature=0)\n    classification_chain = (\n        ChatPromptTemplate.from_messages([\n            (\"system\", \"Classify the query as 'question', 'command', or 'chat'.\"),\n            (\"human\", \"{query}\")\n        ])\n        | llm\n        | (lambda x: {\"classification\": x.content.strip().lower()})\n    )\n    \n    # Define a graph state\n    class ProcessingState(TypedDict):\n        query: str\n        classification: str\n        response: str\n    \n    # Create graph nodes\n    def handle_question(state: ProcessingState) -> Dict:\n        response_chain = ChatPromptTemplate.from_template(\n            \"Here's an answer to your question: {query}\"\n        ) | llm\n        return {\"response\": response_chain.invoke({\"query\": state[\"query\"]}).content}\n    \n    def handle_command(state: ProcessingState) -> Dict:\n        response_chain = ChatPromptTemplate.from_template(\n            \"I'll execute this command: {query}\"\n        ) | llm\n        return {\"response\": response_chain.invoke({\"query\": state[\"query\"]}).content}\n    \n    def handle_chat(state: ProcessingState) -> Dict:\n        response_chain = ChatPromptTemplate.from_template(\n            \"Let's chat about: {query}\"\n        ) | llm\n        return {\"response\": response_chain.invoke({\"query\": state[\"query\"]}).content}\n    \n    # Routing function\n    def route(state: ProcessingState) -> str:\n        classification = state[\"classification\"]\n        if \"question\" in classification:\n            return \"question\"\n        elif \"command\" in classification:\n            return \"command\"\n        else:\n            return \"chat\"\n    \n    # Create the graph\n    graph = StateGraph(ProcessingState)\n    \n    # Add nodes\n    graph.add_node(\"classify\", lambda state: classification_chain.invoke(state))\n    graph.add_node(\"question\", handle_question)\n    graph.add_node(\"command\", handle_command)\n    graph.add_node(\"chat\", handle_chat)\n    \n    # Add edges\n    graph.add_edge(\"classify\", route)\n    graph.add_edge(\"question\", END)\n    graph.add_edge(\"command\", END)\n    graph.add_edge(\"chat\", END)\n    \n    # Set the entry point\n    graph.set_entry_point(\"classify\")\n    \n    return graph.compile()\n\npractical example of building a multi agent system using both frameworks (Langchain+LangGraph )\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import Dict, TypedDict, List, Tuple, Annotated, Literal, Optional\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.output_parsers import PydanticOutputParser\nfrom langchain.tools import Tool\nfrom langchain.agents import AgentExecutor, create_react_agent\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom pydantic import BaseModel, Field\nimport json\n\n# Pattern 1: Agent-based LangGraph Node\n# =====================================\n\n# Define a state that includes chat history\nclass AgentGraphState(TypedDict):\n    messages: List[Union[HumanMessage, AIMessage]]\n    agent_outcome: Optional[str]\n    final_response: Optional[str]\n\n# Create a simple calculator tool\ndef calculator(expression: str) -> float:\n    \"\"\"Evaluate a mathematical expression.\"\"\"\n    try:\n        return eval(expression)\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Create LangChain tools\ntools = [\n    Tool.from_function(\n        func=calculator,\n        name=\"Calculator\",\n        description=\"Useful for performing math calculations\"\n    )\n]\n\n# Create an agent using LangChain's components\ndef create_langchain_agent():\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n    \n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"You are a helpful assistant with access to tools.\"),\n        MessagesPlaceholder(variable_name=\"messages\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n    ])\n    \n    agent = create_react_agent(llm, tools, prompt)\n    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n    \n    return agent_executor\n\n# LangGraph node that uses a LangChain agent\ndef run_agent(state: AgentGraphState) -> Dict:\n    # Extract the last human message\n    last_human = next((msg for msg in reversed(state[\"messages\"]) \n                      if isinstance(msg, HumanMessage)), None)\n    \n    if not last_human:\n        return {\"agent_outcome\": \"No human message found\"}\n    \n    # Run the agent\n    agent_executor = create_langchain_agent()\n    result = agent_executor.invoke({\"input\": last_human.content})\n    \n    return {\"agent_outcome\": result[\"output\"]}\n\n# Format final response\ndef format_response(state: AgentGraphState) -> Dict:\n    return {\"final_response\": f\"Agent result: {state['agent_outcome']}\"}\n\n# Pattern 2: Streaming with LangChain and LangGraph\n# ================================================\n\n# Define a streaming version of our RAG architecture\ndef create_streaming_rag_graph():\n    from langchain_core.runnables import RunnableConfig\n    \n    # Similar setup as before but with streaming\n    class StreamingState(TypedDict):\n        query: str\n        context: str\n        response: str\n    \n    def retrieve(state: StreamingState) -> Dict:\n        # Simulating document retrieval\n        if \"France\" in state[\"query\"]:\n            return {\"context\": \"Paris is the capital of France. It's known for the Eiffel Tower.\"}\n        else:\n            return {\"context\": \"No relevant information found.\"}\n    \n    def generate(state: StreamingState) -> Dict:\n        llm = ChatOpenAI(temperature=0, streaming=True)\n        prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"Answer based on context: {context}\"),\n            (\"human\", \"{query}\")\n        ])\n        \n        chain = prompt | llm\n        \n        # Return a generator that will stream tokens\n        def stream_response():\n            for chunk in chain.stream({\n                \"context\": state[\"context\"],\n                \"query\": state[\"query\"]\n            }):\n                yield {\"response\": chunk.content}\n        \n        return stream_response()\n    \n    # Create a graph with streaming\n    graph = StateGraph(StreamingState)\n    graph.add_node(\"retrieve\", retrieve)\n    graph.add_node(\"generate\", generate)\n    graph.add_edge(\"retrieve\", \"generate\")\n    graph.add_edge(\"generate\", END)\n    graph.set_entry_point(\"retrieve\")\n    \n    return graph.compile()\n\n# Pattern 3: Tool Calling with LangChain and LangGraph\n# ===================================================\n\n# Define structured output schema for tool calling\nclass SearchQuery(BaseModel):\n    query: str = Field(description=\"The search query\")\n    filters: Optional[Dict] = Field(description=\"Optional search filters\", default=None)\n\nclass ResearchAction(BaseModel):\n    action: Literal[\"search\", \"summarize\", \"extract_data\"] = Field(description=\"The type of research action to perform\")\n    search_query: Optional[SearchQuery] = Field(description=\"Search parameters if action is 'search'\", default=None)\n    content_to_process: Optional[str] = Field(description=\"Content to process for summarize or extract_data actions\", default=None)\n\n# Research workflow with tool calling\nclass ResearchState(TypedDict):\n    query: str\n    actions: List[ResearchAction]\n    search_results: Optional[List[str]]\n    final_result: Optional[str]\n\ndef parse_research_actions(state: ResearchState) -> Dict:\n    llm = ChatOpenAI(temperature=0)\n    \n    # Use Pydantic parser with LangChain\n    parser = PydanticOutputParser(pydantic_object=ResearchAction)\n    \n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"\"\"You are a research assistant. Based on the user's query, decide on the next research action to take.\n         {format_instructions}\"\"\"),\n        (\"human\", \"{query}\")\n    ])\n    \n    chain = prompt.format_prompt(\n        query=state[\"query\"],\n        format_instructions=parser.get_format_instructions()\n    ) | llm | parser\n    \n    # Get the next action\n    action = chain.invoke({})\n    \n    return {\"actions\": [action]}\n\ndef execute_search(state: ResearchState) -> Dict:\n    # Simple mock search function\n    actions = state[\"actions\"]\n    last_action = actions[-1] if actions else None\n    \n    if last_action and last_action.action == \"search\" and last_action.search_query:\n        # Mock search results\n        results = [\n            f\"Result 1 for {last_action.search_query.query}\",\n            f\"Result 2 for {last_action.search_query.query}\",\n            f\"Result 3 for {last_action.search_query.query}\"\n        ]\n        return {\"search_results\": results}\n    \n    return {\"search_results\": []}\n\ndef generate_research_summary(state: ResearchState) -> Dict:\n    llm = ChatOpenAI(temperature=0)\n    \n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"Summarize the research results.\"),\n        (\"human\", \"Query: {query}\\nSearch Results: {search_results}\")\n    ])\n    \n    chain = prompt | llm | (lambda x: {\"final_result\": x.content})\n    \n    return chain.invoke(state)\n\n# Pattern 4: Hybrid Chain/Graph Using LCEL\n# =======================================\n\ndef create_hybrid_chain_graph():\n    from langchain_core.runnables import RunnablePassthrough\n    \n    # Create a LangChain chain using LCEL\n    llm = ChatOpenAI(temperature=0)\n    classification_chain = (\n        ChatPromptTemplate.from_messages([\n            (\"system\", \"Classify the query as 'question', 'command', or 'chat'.\"),\n            (\"human\", \"{query}\")\n        ])\n        | llm\n        | (lambda x: {\"classification\": x.content.strip().lower()})\n    )\n    \n    # Define a graph state\n    class ProcessingState(TypedDict):\n        query: str\n        classification: str\n        response: str\n    \n    # Create graph nodes\n    def handle_question(state: ProcessingState) -> Dict:\n        response_chain = ChatPromptTemplate.from_template(\n            \"Here's an answer to your question: {query}\"\n        ) | llm\n        return {\"response\": response_chain.invoke({\"query\": state[\"query\"]}).content}\n    \n    def handle_command(state: ProcessingState) -> Dict:\n        response_chain = ChatPromptTemplate.from_template(\n            \"I'll execute this command: {query}\"\n        ) | llm\n        return {\"response\": response_chain.invoke({\"query\": state[\"query\"]}).content}\n    \n    def handle_chat(state: ProcessingState) -> Dict:\n        response_chain = ChatPromptTemplate.from_template(\n            \"Let's chat about: {query}\"\n        ) | llm\n        return {\"response\": response_chain.invoke({\"query\": state[\"query\"]}).content}\n    \n    # Routing function\n    def route(state: ProcessingState) -> str:\n        classification = state[\"classification\"]\n        if \"question\" in classification:\n            return \"question\"\n        elif \"command\" in classification:\n            return \"command\"\n        else:\n            return \"chat\"\n    \n    # Create the graph\n    graph = StateGraph(ProcessingState)\n    \n    # Add nodes\n    graph.add_node(\"classify\", lambda state: classification_chain.invoke(state))\n    graph.add_node(\"question\", handle_question)\n    graph.add_node(\"command\", handle_command)\n    graph.add_node(\"chat\", handle_chat)\n    \n    # Add edges\n    graph.add_edge(\"classify\", route)\n    graph.add_edge(\"question\", END)\n    graph.add_edge(\"command\", END)\n    graph.add_edge(\"chat\", END)\n    \n    # Set the entry point\n    graph.set_entry_point(\"classify\")\n    \n    return graph.compile()\nLet's also look at a real-world implementation that combines LangChain tools with LangGraph's supervision capabilities:\n\nHuman in the LOOP supervised Tool Calling \nfrom langgraph.graph import StateGraph, END\nfrom typing import Dict, TypedDict, List, Annotated, Literal, Tuple, Any, Optional, Callable\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage, FunctionMessage\nfrom langchain.tools import BaseTool, StructuredTool, tool\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nimport json\nimport uuid\nfrom datetime import datetime\n\n# Define the state structure for a human-in-the-loop application\nclass SupervisedState(TypedDict):\n    messages: List[Any]\n    pending_tool_calls: List[Dict[str, Any]]\n    tool_results: List[Dict[str, Any]]\n    requires_approval: bool\n    human_approved: bool\n    human_feedback: Optional[str]\n    final_response: Optional[str]\n\n# Define our tools using LangChain's tool decorators\n@tool\ndef search_database(query: str) -> str:\n    \"\"\"\n    Search a database for information based on a query.\n    This is a simulated tool - in a real app this would query an actual database.\n    \"\"\"\n    # Simulate database search results\n    fake_db = {\n        \"price\": \"The product costs $199.99\",\n        \"availability\": \"The product will be back in stock on May 15, 2025\",\n        \"specifications\": \"The product dimensions are 10x15x5 inches and it weighs 3.5 pounds\",\n        \"warranty\": \"The product comes with a 2-year limited warranty\"\n    }\n    \n    if any(keyword in query.lower() for keyword in fake_db.keys()):\n        for keyword, info in fake_db.items():\n            if keyword in query.lower():\n                return info\n    return \"No relevant information found in the database.\"\n\n@tool\ndef schedule_action(action: str, date: str) -> str:\n    \"\"\"\n    Schedule an action for a specific date.\n    \"\"\"\n    try:\n        # Validate date format\n        datetime.strptime(date, \"%Y-%m-%d\")\n        return f\"Successfully scheduled '{action}' for {date}\"\n    except ValueError:\n        return \"Error: Date must be in YYYY-MM-DD format\"\n\n@tool\ndef calculate_discount(original_price: float, discount_percentage: float) -> str:\n    \"\"\"\n    Calculate the final price after applying a discount.\n    \"\"\"\n    if discount_percentage < 0 or discount_percentage > 100:\n        return \"Error: Discount percentage must be between 0 and 100\"\n    \n    discounted_price = original_price * (1 - discount_percentage / 100)\n    return f\"Original price: ${original_price:.2f}, Discount: {discount_percentage}%, Final price: ${discounted_price:.2f}\"\n\n# Tool caller agent using LangChain\ndef create_tool_calling_agent():\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n    tools = [search_database, schedule_action, calculate_discount]\n    \n    prompt = ChatPromptTemplate.from_messages([\n        SystemMessage(content=\"\"\"You are a helpful assistant with access to tools.\n        Use these tools to provide the best possible answer to the user's request.\n        If you need to use multiple tools to answer the query completely, do so.\"\"\"),\n        MessagesPlaceholder(variable_name=\"messages\")\n    ])\n    \n    from langchain.agents import create_openai_functions_agent\n    agent = create_openai_functions_agent(llm, tools, prompt)\n    \n    from langchain.agents import AgentExecutor\n    return AgentExecutor(agent=agent, tools=tools, verbose=True)\n\n# LangGraph nodes\ndef process_user_input(state: SupervisedState) -> Dict:\n    # Get the last human message\n    last_message = state[\"messages\"][-1]\n    if not isinstance(last_message, HumanMessage):\n        return state\n    \n    agent = create_tool_calling_agent()\n    \n    # Use LangChain's agent to determine needed tools\n    result = agent.plan(state[\"messages\"])\n    \n    # Extract tool calls from the agent's response\n    pending_tool_calls = []\n    for tool_call in result.tool_calls:\n        pending_tool_calls.append({\n            \"id\": str(uuid.uuid4()),\n            \"name\": tool_call.name,\n            \"arguments\": tool_call.args,\n            \"description\": tool_call.description if hasattr(tool_call, \"description\") else \"\",\n            \"high_risk\": tool_call.name in [\"schedule_action\"]  # Flag certain tools as high risk\n        })\n    \n    # Determine if approval is needed\n    requires_approval = any(call[\"high_risk\"] for call in pending_tool_calls)\n    \n    return {\n        \"pending_tool_calls\": pending_tool_calls,\n        \"requires_approval\": requires_approval,\n        \"human_approved\": not requires_approval  # Auto-approve if not high risk\n    }\n\ndef wait_for_approval(state: SupervisedState) -> Dict:\n    # This is a placeholder node - in a real application,\n    # this would integrate with a UI to get human approval\n    return state\n\ndef execute_tools(state: SupervisedState) -> Dict:\n    tool_results = []\n    \n    for call in state[\"pending_tool_calls\"]:\n        tool_name = call[\"name\"]\n        arguments = call[\"arguments\"]\n        \n        # Find the appropriate tool\n        if tool_name == \"search_database\":\n            result = search_database(query=arguments[\"query\"])\n        elif tool_name == \"schedule_action\":\n            result = schedule_action(action=arguments[\"action\"], date=arguments[\"date\"])\n        elif tool_name == \"calculate_discount\":\n            result = calculate_discount(\n                original_price=float(arguments[\"original_price\"]), \n                discount_percentage=float(arguments[\"discount_percentage\"])\n            )\n        else:\n            result = f\"Unknown tool: {tool_name}\"\n        \n        # Record the result\n        tool_results.append({\n            \"call_id\": call[\"id\"],\n            \"tool_name\": tool_name,\n            \"arguments\": arguments,\n            \"result\": result\n        })\n    \n    # Add tool results to the state\n    return {\"tool_results\": tool_results}\n\ndef generate_response(state: SupervisedState) -> Dict:\n    llm = ChatOpenAI(temperature=0)\n    \n    # Prepare messages including tool results\n    tool_result_messages = []\n    for result in state[\"tool_results\"]:\n        content = f\"Tool: {result['tool_name']}\\nArguments: {json.dumps(result['arguments'])}\\nResult: {result['result']}\"\n        tool_result_messages.append(FunctionMessage(content=content, name=result['tool_name']))\n    \n    # Create prompt\n    prompt = ChatPromptTemplate.from_messages([\n        SystemMessage(content=\"You are a helpful assistant. Use the tool results to provide a comprehensive answer.\"),\n        MessagesPlaceholder(variable_name=\"messages\"),\n        SystemMessage(content=\"Here are the results from the tools you used:\"),\n        *tool_result_messages,\n        SystemMessage(content=\"Now provide your final answer, incorporating all relevant tool results.\")\n    ])\n    \n    # Generate response\n    chain = prompt | llm\n    response = chain.invoke({\"messages\": state[\"messages\"]})\n    \n    # Add response to messages and set final_response\n    return {\n        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n        \"final_response\": response.content\n    }\n\n# Router functions\ndef approval_router(state: SupervisedState) -> str:\n    if state[\"requires_approval\"]:\n        if state[\"human_approved\"]:\n            return \"execute\"\n        else:\n            return \"wait_approval\"\n    else"
  },
  {
    "id": "1cbf0b00-3427-80d9-95a6-ddc7185e6dc8",
    "title": "Arrays , Strings and STL ",
    "content": "\n1. Array (Built-in C++)\nExplanation\nAn array is a collection of elements of the same type stored in contiguous memory locations. Each element can be accessed directly using an index. Arrays in C++ have a fixed size that must be defined at compile time.\nUse Cases\nStoring a fixed collection of similar items (e.g., student grades, monthly temperatures)\nImplementation of other data structures like stacks, queues, and hash tables\nMatrix and grid representations for games or mathematical operations\nLookup tables for fast access to predetermined values\nC++ Code Implementation\n#include <iostream>\nusing namespace std;\n\nint main() {\n    // Declaration and initialization\n    int numbers[5] = {10, 20, 30, 40, 50};\n\n    // Accessing elements\n    cout << \"Third element: \" << numbers[2] << endl; // Output: 30\n\n    // Modifying elements\n    numbers[1] = 25;\n    cout << \"New second element: \" << numbers[1] << endl; // Output: 25\n\n    // Traversing an array\n    cout << \"All elements: \";\n    for(int i = 0; i < 5; i++) {\n        cout << numbers[i] << \" \";\n    }\n    cout << endl;\n\n    // Multi-dimensional array\n    int matrix[3][3] = {\n        {1, 2, 3},\n        {4, 5, 6},\n        {7, 8, 9}\n    };\n\n    cout << \"Matrix element at (1,2): \" << matrix[1][2] << endl; // Output: 6\n\n    return 0;\n}\n\nOperations and Time Complexity\n[table block]\n2. String (Built-in C++)\nExplanation\nA string in C++ is a sequence of characters represented by the std::string class from the Standard Library. It provides a convenient way to handle text data and perform string operations.\nUse Cases\nText processing and manipulation\nUser input/output in applications\nStoring names, messages, or any textual data\nPattern matching and substring operations\nC++ Code Implementation\n#include <iostream>\n#include <string>\nusing namespace std;\n\nint main() {\n    // Declaration and initialization\n    string greeting = \"Hello, World!\";\n    string empty_string; // Empty string\n\n    // String assignment\n    string name = \"C++ Programming\";\n\n    // String concatenation\n    string full_greeting = greeting + \" Welcome to \" + name;\n    cout << \"Concatenated string: \" << full_greeting << endl;\n\n    // String length\n    cout << \"String length: \" << greeting.length() << endl; // Also can use greeting.size()\n\n    // Accessing characters\n    cout << \"First character: \" << greeting[0] << endl;    // Using [] operator\n    cout << \"Last character: \" << greeting.at(greeting.length() - 1) << endl; // Using at() method\n\n    // Substring\n    string sub = greeting.substr(0, 5); // Get \"Hello\"\n    cout << \"Substring: \" << sub << endl;\n\n    // Finding in string\n    size_t pos = greeting.find(\"World\");\n    if (pos != string::npos) {\n        cout << \"'World' found at position: \" << pos << endl;\n    }\n\n    // Modifying strings\n    greeting.replace(7, 5, \"C++\"); // Replace \"World\" with \"C++\"\n    cout << \"Modified string: \" << greeting << endl;\n\n    // Comparing strings\n    string str1 = \"Apple\";\n    string str2 = \"Banana\";\n    if (str1 < str2) { // Lexicographical comparison\n        cout << str1 << \" comes before \" << str2 << endl;\n    }\n\n    return 0;\n}\n\nOperations and Time Complexity\n[table block]\n3. Standard Template Library (STL)\n3.1. Vector (Dynamic Array)\nExplanation\nA vector is a dynamic array that can resize itself automatically when elements are inserted or deleted. It's implemented as a template class in the STL and provides dynamic memory management.\nUse Cases\nWhen array size is not known at compile time\nWhen frequent insertions and deletions are needed\nBuilding lists of items that may grow or shrink\nImplementing stacks or other data structures\nC++ Code Implementation\n#include <iostream>\n#include <vector>\nusing namespace std;\n\nint main() {\n    // Declaration and initialization\n    vector<int> numbers;         // Empty vector\n    vector<int> preset = {10, 20, 30, 40, 50}; // Vector with initial elements\n\n    // Adding elements\n    numbers.push_back(5);        // Add to the end\n    numbers.push_back(10);\n    numbers.push_back(15);\n\n    // Inserting elements at specific position\n    numbers.insert(numbers.begin() + 1, 7); // Insert 7 at index 1\n\n    // Accessing elements\n    cout << \"First element: \" << numbers[0] << endl;    // Using [] operator\n    cout << \"Second element: \" << numbers.at(1) << endl; // Using at() method\n\n    // Size operations\n    cout << \"Vector size: \" << numbers.size() << endl;\n    cout << \"Vector capacity: \" << numbers.capacity() << endl;\n\n    // Iterating through a vector\n    cout << \"All elements: \";\n    for(int num : numbers) {\n        cout << num << \" \";\n    }\n    cout << endl;\n\n    // Using iterators\n    cout << \"Using iterators: \";\n    for(auto it = numbers.begin(); it != numbers.end(); ++it) {\n        cout << *it << \" \";\n    }\n    cout << endl;\n\n    // Removing elements\n    numbers.pop_back();        // Remove the last element\n    numbers.erase(numbers.begin()); // Remove the first element\n\n    // Checking if empty\n    if(!numbers.empty()) {\n        cout << \"Vector is not empty\" << endl;\n    }\n\n    // Clearing the vector\n    numbers.clear();\n    cout << \"After clear, size: \" << numbers.size() << endl;\n\n    return 0;\n}\n\nOperations and Time Complexity\n[table block]\n3.2. Stack (LIFO Structure)\nExplanation\nA stack is a Last-In-First-Out (LIFO) data structure where the most recently added element is the first one to be removed. In C++, it's implemented as a container adapter in the STL.\nUse Cases\nFunction call management in program execution\nExpression evaluation and syntax parsing\nUndo mechanisms in applications\nDepth-first search algorithm implementation\nBacktracking algorithms\nC++ Code Implementation\n#include <iostream>\n#include <stack>\nusing namespace std;\n\nint main() {\n    // Declaration\n    stack<int> myStack;\n\n    // Pushing elements\n    myStack.push(10);\n    myStack.push(20);\n    myStack.push(30);\n\n    // Stack size\n    cout << \"Stack size: \" << myStack.size() << endl;\n\n    // Accessing top element\n    cout << \"Top element: \" << myStack.top() << endl;\n\n    // Popping elements\n    myStack.pop();\n    cout << \"After pop, top element: \" << myStack.top() << endl;\n\n    // Check if stack is empty\n    if(!myStack.empty()) {\n        cout << \"Stack is not empty\" << endl;\n    }\n\n    // Emptying the stack and displaying elements\n    cout << \"Emptying stack: \";\n    while(!myStack.empty()) {\n        cout << myStack.top() << \" \";\n        myStack.pop();\n    }\n    cout << endl;\n\n    return 0;\n}\n\nOperations and Time Complexity\n[table block]\n3.3. Queue (FIFO Structure)\nExplanation\nA queue is a First-In-First-Out (FIFO) data structure where the first added element is the first one to be removed. In C++, it's implemented as a container adapter in the STL.\nUse Cases\nTask scheduling in operating systems\nBuffer for data streams\nBreadth-first search algorithm implementation\nPrint job management\nCustomer service systems\nC++ Code Implementation\n#include <iostream>\n#include <queue>\nusing namespace std;\n\nint main() {\n    // Declaration\n    queue<int> myQueue;\n\n    // Adding elements (enqueue)\n    myQueue.push(10);\n    myQueue.push(20);\n    myQueue.push(30);\n\n    // Queue size\n    cout << \"Queue size: \" << myQueue.size() << endl;\n\n    // Accessing front and back elements\n    cout << \"Front element: \" << myQueue.front() << endl;\n    cout << \"Back element: \" << myQueue.back() << endl;\n\n    // Removing elements (dequeue)\n    myQueue.pop();\n    cout << \"After pop, front element: \" << myQueue.front() << endl;\n\n    // Check if queue is empty\n    if(!myQueue.empty()) {\n        cout << \"Queue is not empty\" << endl;\n    }\n\n    // Emptying the queue and displaying elements\n    cout << \"Emptying queue: \";\n    while(!myQueue.empty()) {\n        cout << myQueue.front() << \" \";\n        myQueue.pop();\n    }\n    cout << endl;\n\n    return 0;\n}\n\nOperations and Time Complexity\n[table block]\n3.4. Set (Unique Elements)\nExplanation\nA set is an ordered container that stores unique elements. It's typically implemented as a balanced binary search tree (like a Red-Black tree) in the STL, ensuring fast search, insertion, and deletion operations.\nUse Cases\nMaintaining a collection of unique values\nSet operations (union, intersection, difference)\nChecking for duplicates in a list\nImplementing dictionaries or maps\nAuto-sorting elements in ascending order\nC++ Code Implementation\n#include <iostream>\n#include <set>\nusing namespace std;\n\nint main() {\n    // Declaration\n    set<int> mySet;\n\n    // Inserting elements\n    mySet.insert(30);\n    mySet.insert(10);\n    mySet.insert(20);\n    mySet.insert(10); // Duplicate, will be ignored\n\n    // Set size\n    cout << \"Set size: \" << mySet.size() << endl;\n\n    // Iterating through the set\n    cout << \"Set elements (automatically sorted): \";\n    for(const int& element : mySet) {\n        cout << element << \" \";\n    }\n    cout << endl;\n\n    // Finding elements\n    auto it = mySet.find(20);\n    if(it != mySet.end()) {\n        cout << \"Found element: \" << *it << endl;\n    }\n\n    // Checking if an element exists\n    if(mySet.count(15) == 0) {\n        cout << \"15 is not in the set\" << endl;\n    }\n\n    // Erasing elements\n    mySet.erase(10);\n\n    // Using lower_bound and upper_bound\n    auto lower = mySet.lower_bound(20); // Iterator to first element >= 20\n    auto upper = mySet.upper_bound(20); // Iterator to first element > 20\n\n    cout << \"Lower bound of 20: \" << *lower << endl;\n    if(upper != mySet.end()) {\n        cout << \"Upper bound of 20: \" << *upper << endl;\n    }\n\n    // Clearing the set\n    mySet.clear();\n    if(mySet.empty()) {\n        cout << \"Set is now empty\" << endl;\n    }\n\n    return 0;\n}\n\nOperations and Time Complexity\n[table block]\n3.5. Map (Key-Value Pairs)\nExplanation\nA map is an ordered container that stores key-value pairs, where each key is unique. Like sets, maps are typically implemented as balanced binary search trees in the STL, providing efficient lookup, insertion, and deletion operations.\nUse Cases\nDictionary implementations\nCaching mechanisms\nFrequency counters\nSymbol tables in compilers\nSparse matrices or graphs\nC++ Code Implementation\n#include <iostream>\n#include <map>\n#include <string>\nusing namespace std;\n\nint main() {\n    // Declaration\n    map<string, int> studentScores;\n\n    // Inserting key-value pairs\n    studentScores[\"Alice\"] = 95;\n    studentScores[\"Bob\"] = 87;\n    studentScores.insert(make_pair(\"Charlie\", 91));\n\n    // Accessing values using keys\n    cout << \"Bob's score: \" << studentScores[\"Bob\"] << endl;\n\n    // Using at() - throws exception if key not found\n    try {\n        cout << \"Charlie's score: \" << studentScores.at(\"Charlie\") << endl;\n        cout << \"David's score: \" << studentScores.at(\"David\") << endl; // Will throw exception\n    } catch(const out_of_range& e) {\n        cout << \"Exception: \" << e.what() << endl;\n    }\n\n    // Checking if a key exists\n    if(studentScores.count(\"Alice\") > 0) {\n        cout << \"Alice's score exists in the map\" << endl;\n    }\n\n    // Finding an element\n    auto it = studentScores.find(\"Bob\");\n    if(it != studentScores.end()) {\n        cout << \"Found \" << it->first << \" with score \" << it->second << endl;\n    }\n\n    // Iterating through the map\n    cout << \"All student scores: \" << endl;\n    for(const auto& pair : studentScores) {\n        cout << pair.first << \": \" << pair.second << endl;\n    }\n\n    // Removing an element\n    studentScores.erase(\"Bob\");\n\n    // Size of map\n    cout << \"Map size after removal: \" << studentScores.size() << endl;\n\n    // Clearing the map\n    studentScores.clear();\n\n    return 0;\n}\n\nOperations and Time Complexity\n[table block]\n3.6. Unordered Set (Hash Table)\nExplanation\nAn unordered set is a container that stores unique elements in no particular order. It uses a hash table implementation, providing faster average-case performance for insertions, deletions, and lookups compared to ordered sets.\nUse Cases\nFast membership testing\nRemoving duplicates from a collection\nImplementing hash-based algorithms\nWhen order doesn't matter but lookup speed does\nCaching unique values\nC++ Code Implementation\n#include <iostream>\n#include <unordered_set>\n#include <string>\nusing namespace std;\n\nint main() {\n    // Declaration\n    unordered_set<string> words;\n\n    // Inserting elements\n    words.insert(\"apple\");\n    words.insert(\"banana\");\n    words.insert(\"cherry\");\n    words.insert(\"apple\"); // Duplicates are ignored\n\n    // Size of the unordered_set\n    cout << \"Size: \" << words.size() << endl;\n\n    // Checking if element exists\n    if(words.count(\"banana\") > 0) {\n        cout << \"Found 'banana'\" << endl;\n    }\n\n    // Finding an element\n    auto it = words.find(\"cherry\");\n    if(it != words.end()) {\n        cout << \"Found: \" << *it << endl;\n    }\n\n    // Iterating through elements (in no particular order)\n    cout << \"All elements: \";\n    for(const auto& word : words) {\n        cout << word << \" \";\n    }\n    cout << endl;\n\n    // Removing an element\n    words.erase(\"banana\");\n\n    // Load factor and bucket information\n    cout << \"Load factor: \" << words.load_factor() << endl;\n    cout << \"Bucket count: \" << words.bucket_count() << endl;\n\n    // Clearing all elements\n    words.clear();\n\n    return 0;\n}\n\nOperations and Time Complexity\n[table block]\n3.7. Unordered Map (Hash Table)\nExplanation\nAn unordered map is a container that stores key-value pairs with unique keys in no particular order. Like the unordered set, it uses a hash table implementation for faster average lookups.\nUse Cases\nFast lookups by key\nCaching computed values\nFrequency counting\nSymbol tables where order is not important\nImplementing associative arrays\nC++ Code Implementation\n#include <iostream>\n#include <unordered_map>\n#include <string>\nusing namespace std;\n\nint main() {\n    // Declaration\n    unordered_map<string, int> wordFrequency;\n\n    // Inserting key-value pairs\n    wordFrequency[\"hello\"] = 1;\n    wordFrequency[\"world\"] = 2;\n    wordFrequency.insert(make_pair(\"goodbye\", 3));\n\n    // Incrementing values\n    wordFrequency[\"hello\"]++;\n\n    // Accessing values\n    cout << \"Frequency of 'hello': \" << wordFrequency[\"hello\"] << endl;\n\n    // Using at() - throws exception if key not found\n    try {\n        cout << \"Frequency of 'world': \" << wordFrequency.at(\"world\") << endl;\n    } catch(const out_of_range& e) {\n        cout << \"Key not found!\" << endl;\n    }\n\n    // Finding an element\n    if(wordFrequency.find(\"goodbye\") != wordFrequency.end()) {\n        cout << \"Found 'goodbye'\" << endl;\n    }\n\n    // Iterating through all key-value pairs\n    cout << \"All word frequencies:\" << endl;\n    for(const auto& pair : wordFrequency) {\n        cout << pair.first << \": \" << pair.second << endl;\n    }\n\n    // Checking if a key exists\n    if(wordFrequency.count(\"hello\") > 0) {\n        cout << \"'hello' exists in the map\" << endl;\n    }\n\n    // Removing an element\n    wordFrequency.erase(\"world\");\n\n    // Hash policy information\n    cout << \"Load factor: \" << wordFrequency.load_factor() << endl;\n    cout << \"Bucket count: \" << wordFrequency.bucket_count() << endl;\n\n    return 0;\n}\n\nOperations and Time Complexity\n[table block]\n3.8. List (Doubly Linked List)\nExplanation\nA list in C++ STL is implemented as a doubly linked list, where each element contains a value and pointers to both the previous and next elements. This allows for efficient insertions and deletions anywhere in the list.\nUse Cases\nWhen frequent insertions/deletions are needed anywhere in the sequence\nImplementing other data structures like queues and stacks\nManaging large elements where moving data is expensive\nSituations where elements need to be referenced by iterators\nC++ Code Implementation\n#include <iostream>\n#include <list>\nusing namespace std;\n\nint main() {\n    // Declaration and initialization\n    list<int> myList;\n    list<int> anotherList = {1, 2, 3, 4, 5};\n\n    // Adding elements\n    myList.push_back(10);    // Add to the end\n    myList.push_front(5);    // Add to the beginning\n\n    // Insert at specific position\n    auto it = myList.begin();\n    advance(it, 1);          // Move to second position\n    myList.insert(it, 7);    // Insert 7 at second position\n\n    // Accessing elements (no direct random access)\n    cout << \"First element: \" << myList.front() << endl;\n    cout << \"Last element: \" << myList.back() << endl;\n\n    // Traversing the list\n    cout << \"List elements: \";\n    for(const auto& element : myList) {\n        cout << element << \" \";\n    }\n    cout << endl;\n\n    // Removing elements\n    myList.pop_front();      // Remove from the beginning\n    myList.pop_back();       // Remove from the end\n\n    // Remove by value\n    myList.remove(7);        // Removes all occurrences of 7\n\n    // Size of the list\n    cout << \"List size: \" << myList.size() << endl;\n\n    // Splicing lists (moving elements from one list to another)\n    myList.splice(myList.begin(), anotherList);\n\n    cout << \"After splicing: \";\n    for(const auto& element : myList) {\n        cout << element << \" \";\n    }\n    cout << endl;\n\n    // Sort the list\n    myList.sort();\n\n    // Remove duplicate consecutive elements\n    myList.unique();\n\n    // Reverse the list\n    myList.reverse();\n\n    cout << \"After sort, unique, and reverse: \";\n    for(const auto& element : myList) {\n        cout << element << \" \";\n    }\n    cout << endl;\n\n    return 0;\n}\n\nOperations and Time Complexity\n[table block]\n3.9. Deque (Double-Ended Queue)\nExplanation\nA deque (double-ended queue) is a sequence container that allows fast insertions and deletions at both its beginning and end. Unlike vectors, it doesn't guarantee contiguous storage allocation.\nUse Cases\nImplementing both stack and queue functionality\nSliding window problems in algorithms\nWork stealing algorithms\nWhen efficient insertion/removal at both ends is needed\nBuffer implementations\nC++ Code Implementation\n#include <iostream>\n#include <deque>\nusing namespace std;\n\nint main() {\n    // Declaration and initialization\n    deque<int> myDeque;\n    deque<int> anotherDeque = {1, 2, 3, 4, 5};\n\n    // Adding elements\n    myDeque.push_back(30);    // Add to the end\n    myDeque.push_front(10);   // Add to the beginning\n    myDeque.insert(myDeque.begin() + 1, 20); // Insert at position\n\n    // Accessing elements\n    cout << \"First element: \" << myDeque.front() << endl;\n    cout << \"Last element: \" << myDeque.back() << endl;\n    cout << \"Element at index 1: \" << myDeque[1] << endl;    // Using [] operator\n    cout << \"Element at index 2: \" << myDeque.at(2) << endl; // Using at() method\n\n    // Size information\n    cout << \"Size: \" << myDeque.size() << endl;\n\n    // Iterating through elements\n    cout << \"All elements: \";\n    for(const auto& element : myDeque) {\n        cout << element << \" \";\n    }\n    cout << endl;\n\n    // Removing elements\n    myDeque.pop_front();     // Remove from the beginning\n    myDeque.pop_back();      // Remove from the end\n\n    cout << \"After removals: \";\n    for(const auto& element : myDeque) {\n        cout << element << \" \";\n    }\n    cout << endl;\n\n    // Resizing\n    myDeque.resize(5, 100);  // Resize to 5 elements, fill new positions with 100\n\n    cout << \"After resize: \";\n    for(const auto& element : myDeque) {\n        cout << element << \" \";\n    }\n    cout << endl;\n\n    // Clearing\n    myDeque.clear();\n\n    return 0;\n}\n\nOperations and Time Complexity\n[table block]\n3.10 Multiset \nExplanation \n\nA multiset is an ordered container like set but allows duplicate elements. It keeps elements sorted according to a comparison function, with logarithmic complexity for most operations.\nUse Cases\nWhen you need to track frequency of elements while maintaining order\nImplementing multi-occurrence counters with automatic sorting\nStatistical applications requiring sorted collections with duplicates\nC++ Code Implementation\n#include <iostream>\n#include <set>\n\nint main() {\n    std::multiset<int> ms;\n\n    // Insert elements (duplicates allowed)\n    ms.insert(10);\n    ms.insert(20);\n    ms.insert(10);  // Duplicate allowed\n    ms.insert(30);\n\n    // Count occurrences\n    std::cout << \"Count of 10: \" << ms.count(10) << std::endl;  // Outputs: 2\n\n    // Iterate through elements\n    for (const auto& element : ms) {\n        std::cout << element << \" \";  // Outputs: 10 10 20 30\n    }\n    std::cout << std::endl;\n\n    // Find elements\n    auto it = ms.find(10);  // Returns iterator to first occurrence of 10\n\n    // Erase specific value (removes all occurrences)\n    ms.erase(10);\n\n    // Erase by iterator (removes only one occurrence)\n    if (it != ms.end()) {\n        ms.erase(it);  // Removes only one occurrence\n    }\n\n    return 0;\n}\n\nOperations and Time Complexity\nInsert: O(log n)\nErase (by value): O(log n + count) where count is the number of occurrences\nErase (by iterator): O(log n)\nFind: O(log n)\nCount: O(log n + count) where count is the number of occurrences\nLower/Upper bound: O(log n)\n3.11. Multimap\nExplanation\nA multimap is an ordered associative container that stores key-value pairs while allowing multiple entries with the same key. Elements are sorted by keys using a comparison function.\nUse Cases\nDictionary implementations where multiple values can be associated with one key\nImplementing many-to-many relationships\nCross-reference systems\nC++ Code Implementation\n#include <iostream>\n#include <map>\n#include <string>\n\nint main() {\n    std::multimap<std::string, int> grades;\n\n    // Insert key-value pairs (duplicates allowed)\n    grades.insert({\"Alice\", 85});\n    grades.insert({\"Bob\", 90});\n    grades.insert({\"Alice\", 92});  // Same key, different value\n    grades.insert({\"Charlie\", 78});\n\n    // Count entries with specific key\n    std::cout << \"Alice's grade count: \" << grades.count(\"Alice\") << std::endl;  // Outputs: 2\n\n    // Find all entries with a specific key\n    auto range = grades.equal_range(\"Alice\");\n    std::cout << \"Alice's grades: \";\n    for (auto it = range.first; it != range.second; ++it) {\n        std::cout << it->second << \" \";  // Outputs: 85 92\n    }\n    std::cout << std::endl;\n\n    // Iterate through all entries\n    for (const auto& entry : grades) {\n        std::cout << entry.first << \": \" << entry.second << std::endl;\n    }\n\n    // Erase by key (removes all occurrences)\n    grades.erase(\"Alice\");\n\n    return 0;\n}\n\nOperations and Time Complexity\nInsert: O(log n)\nErase (by key): O(log n + count) where count is the number of occurrences\nErase (by iterator): O(log n)\nFind: O(log n)\nEqual_range: O(log n)\nCount: O(log n + count)\n3. 12Priority Queue\nExplanation\nA priority queue is a container adapter that provides constant time lookup of the largest (or smallest) element, at the expense of logarithmic insertion and extraction. It's implemented as a heap.\nUse Cases\nTask scheduling by priority\nDijkstra's algorithm and other graph algorithms\nEvent-driven simulations\nHeap sort implementation\nC++ Code Implementation\n#include <iostream>\n#include <queue>\n#include <vector>\n\nint main() {\n    // Max priority queue (default)\n    std::priority_queue<int> maxPQ;\n\n    // Push elements\n    maxPQ.push(10);\n    maxPQ.push(30);\n    maxPQ.push(20);\n\n    // View top element (highest priority)\n    std::cout << \"Top element: \" << maxPQ.top() << std::endl;  // Outputs: 30\n\n    // Pop top element\n    maxPQ.pop();\n    std::cout << \"New top after pop: \" << maxPQ.top() << std::endl;  // Outputs: 20\n\n    // Min priority queue (using comparison function)\n    std::priority_queue<int, std::vector<int>, std::greater<int>> minPQ;\n\n    // Push elements to min queue\n    minPQ.push(10);\n    minPQ.push(30);\n    minPQ.push(20);\n\n    std::cout << \"Min queue top: \" << minPQ.top() << std::endl;  // Outputs: 10\n\n    // Custom comparison using lambda (C++11 and later)\n    auto cmp = [](const std::pair<std::string, int>& a, const std::pair<std::string, int>& b) {\n        return a.second > b.second;  // Sort by value (ascending)\n    };\n\n    std::priority_queue<std::pair<std::string, int>,\n                         std::vector<std::pair<std::string, int>>,\n                         decltype(cmp)> customPQ(cmp);\n\n    customPQ.push({\"Task A\", 3});\n    customPQ.push({\"Task B\", 1});\n    customPQ.push({\"Task C\", 2});\n\n    std::cout << \"Highest priority task: \" << customPQ.top().first << std::endl;  // Outputs: Task B\n\n    return 0;\n}\n\nOperations and Time Complexity\nPush: O(log n)\nPop: O(log n)\nTop (access highest priority): O(1)\nSize: O(1)\nEmpty: O(1)\n3.13. Forward List\nExplanation\nA forward_list is a singly-linked list implementation. It supports fast insertion and removal of elements from anywhere in the container but allows only forward traversal.\nUse Cases\nMemory-constrained environments (uses less overhead than list)\nWhen forward-only traversal is sufficient\nImplementing simple queues or stacks\nWhen frequent insertions at arbitrary positions are needed\nC++ Code Implementation\n#include <iostream>\n#include <forward_list>\n\nint main() {\n    std::forward_list<int> fl = {10, 20, 30};\n\n    // Insert element at front\n    fl.push_front(5);\n\n    // Insert after position\n    auto it = fl.begin();\n    fl.insert_after(it, 15);  // Inserts after first element\n\n    // Iterate and print\n    std::cout << \"Forward list elements: \";\n    for (const auto& val : fl) {\n        std::cout << val << \" \";  // Outputs: 5 15 10 20 30\n    }\n    std::cout << std::endl;\n\n    // Erase after position\n    it = fl.begin();  // Reset iterator to beginning\n    fl.erase_after(it);  // Removes element after first\n\n    // Find element (manual traversal required)\n    it = fl.before_begin();\n    auto prev = it;\n    for (; it != fl.end(); prev = it, ++it) {\n        if (*it == 20) {\n            fl.erase_after(prev);  // Remove 20\n            break;\n        }\n    }\n\n    // Sort and remove duplicates\n    fl.push_front(10);\n    fl.push_front(10);\n    fl.sort();  // Sort elements\n    fl.unique();  // Remove consecutive duplicates\n\n    std::cout << \"After operations: \";\n    for (const auto& val : fl) {\n        std::cout << val << \" \";\n    }\n    std::cout << std::endl;\n\n    return 0;\n}\n"
  },
  {
    "id": "1c8f0b00-3427-8004-87bd-c32d73d54b54",
    "title": "Tree Data Structures in C++",
    "content": "This repository provides implementations and explanations of various tree-based data structures. Each implementation includes both manual and STL-based approaches where applicable.\nTable of Contents\nBinary Tree\nBinary Search Tree (BST)\nAVL Tree\nRed-Black Tre\nB-Trees & B+ Trees\nHeap (Min Heap, Max Heap)\nTrie\nSegment Tree\n[divider block]\nBinary Tree\nIntroduction\nA Binary Tree is a hierarchical data structure where each node has at most two children, referred to as the left child and the right child. Unlike arrays or linked lists, which are linear data structures, trees are non-linear, allowing quicker access to the data.\nUse Cases & Applications\nRepresent hierarchical data (file systems, organization structures)\nExpression trees in compilers\nHuffman coding trees for data compression\nFoundation for more complex tree structures (BST, AVL, etc.)\nAdvantages & Limitations\nAdvantages:\nSimple and intuitive hierarchical structure\nProvides a foundation for more specialized tree structures\nEfficient for specific operations like traversals\nLimitations:\nBasic binary trees don't guarantee balanced structure\nSearch, insertion, and deletion operations can degrade to O(n) in worst case\nNo inherent ordering of elements\nC++ Implementation\nWithout STL\n#include <iostream>\n\n// Node structure for Binary Tree\nstruct TreeNode {\n    int data;\n    TreeNode* left;\n    TreeNode* right;\n\n    // Constructor\n    TreeNode(int val) : data(val), left(nullptr), right(nullptr) {}\n};\n\nclass BinaryTree {\nprivate:\n    TreeNode* root;\n\n    // Helper method for inorder traversal\n    void inorderTraversal(TreeNode* node) {\n        if (node == nullptr) return;\n\n        inorderTraversal(node->left);\n        std::cout << node->data << \" \";\n        inorderTraversal(node->right);\n    }\n\n    // Helper method for preorder traversal\n    void preorderTraversal(TreeNode* node) {\n        if (node == nullptr) return;\n\n        std::cout << node->data << \" \";\n        preorderTraversal(node->left);\n        preorderTraversal(node->right);\n    }\n\n    // Helper method for postorder traversal\n    void postorderTraversal(TreeNode* node) {\n        if (node == nullptr) return;\n\n        postorderTraversal(node->left);\n        postorderTraversal(node->right);\n        std::cout << node->data << \" \";\n    }\n\n    // Helper method to delete the tree\n    void deleteTree(TreeNode* node) {\n        if (node == nullptr) return;\n\n        deleteTree(node->left);\n        deleteTree(node->right);\n        delete node;\n    }\n\npublic:\n    // Constructor\n    BinaryTree() : root(nullptr) {}\n\n    // Destructor\n    ~BinaryTree() {\n        deleteTree(root);\n    }\n\n    // Create a sample binary tree for demonstration\n    void createSampleTree() {\n        root = new TreeNode(1);\n        root->left = new TreeNode(2);\n        root->right = new TreeNode(3);\n        root->left->left = new TreeNode(4);\n        root->left->right = new TreeNode(5);\n        root->right->left = new TreeNode(6);\n        root->right->right = new TreeNode(7);\n    }\n\n    // Traversal methods\n    void inorder() {\n        std::cout << \"Inorder Traversal: \";\n        inorderTraversal(root);\n        std::cout << std::endl;\n    }\n\n    void preorder() {\n        std::cout << \"Preorder Traversal: \";\n        preorderTraversal(root);\n        std::cout << std::endl;\n    }\n\n    void postorder() {\n        std::cout << \"Postorder Traversal: \";\n        postorderTraversal(root);\n        std::cout << std::endl;\n    }\n};\n\nExample Usage\nint main() {\n    BinaryTree tree;\n    tree.createSampleTree();\n\n    // Print traversals\n    tree.inorder();\n    tree.preorder();\n    tree.postorder();\n\n    return 0;\n}\n\nCode Explanation\nTreeNode structure: Represents a node in the binary tree with data and pointers to left and right children.\nBinaryTree class: Manages the tree with methods for:\nThe traversal algorithms are implemented recursively, demonstrating the classic depth-first approach.\nExample Input/Output\nInorder Traversal: 4 2 5 1 6 3 7\nPreorder Traversal: 1 2 4 5 3 6 7\nPostorder Traversal: 4 5 2 6 7 3 1\n\nComplexity Analysis\n[table block]\n[divider block]\nBinary Search Tree (BST)\nIntroduction\nA Binary Search Tree (BST) is a binary tree with the additional property that the left subtree of any node contains only values less than the node's value, and the right subtree contains only values greater than the node's value. This ordering property makes searching efficient.\nUse Cases & Applications\nImplementing dynamic sets and maps/dictionaries\nDatabase indexing\nPriority queues\nSyntax trees in compilers\nDecision-making processes\nAdvantages & Limitations\nAdvantages:\nFast search, insertion, and deletion (average O(log n))\nOrdered structure allowing for efficient in-order traversals\nSimple implementation compared to self-balancing trees\nLimitations:\nPerformance degrades to O(n) in worst case (unbalanced tree)\nNo constant-time operations\nExtra memory for pointers compared to arrays\nC++ Implementation\nWithout STL\n#include <iostream>\n\n// Node structure for BST\nstruct BSTNode {\n    int data;\n    BSTNode* left;\n    BSTNode* right;\n\n    // Constructor\n    BSTNode(int val) : data(val), left(nullptr), right(nullptr) {}\n};\n\nclass BinarySearchTree {\nprivate:\n    BSTNode* root;\n\n    // Helper method for inserting a value\n    BSTNode* insertRecursive(BSTNode* node, int value) {\n        if (node == nullptr) {\n            return new BSTNode(value);\n        }\n\n        if (value < node->data) {\n            node->left = insertRecursive(node->left, value);\n        } else if (value > node->data) {\n            node->right = insertRecursive(node->right, value);\n        }\n\n        return node;\n    }\n\n    // Helper method for searching a value\n    BSTNode* searchRecursive(BSTNode* node, int value) {\n        if (node == nullptr || node->data == value) {\n            return node;\n        }\n\n        if (value < node->data) {\n            return searchRecursive(node->left, value);\n        }\n\n        return searchRecursive(node->right, value);\n    }\n\n    // Helper method to find minimum value node\n    BSTNode* findMinNode(BSTNode* node) {\n        BSTNode* current = node;\n        while (current && current->left != nullptr) {\n            current = current->left;\n        }\n        return current;\n    }\n\n    // Helper method for deleting a node\n    BSTNode* deleteRecursive(BSTNode* node, int value) {\n        if (node == nullptr) return nullptr;\n\n        // Navigate to the node to delete\n        if (value < node->data) {\n            node->left = deleteRecursive(node->left, value);\n        } else if (value > node->data) {\n            node->right = deleteRecursive(node->right, value);\n        } else {\n            // Node with only one child or no child\n            if (node->left == nullptr) {\n                BSTNode* temp = node->right;\n                delete node;\n                return temp;\n            } else if (node->right == nullptr) {\n                BSTNode* temp = node->left;\n                delete node;\n                return temp;\n            }\n\n            // Node with two children\n            BSTNode* temp = findMinNode(node->right);\n            node->data = temp->data;\n            node->right = deleteRecursive(node->right, temp->data);\n        }\n        return node;\n    }\n\n    // Helper method for inorder traversal\n    void inorderTraversal(BSTNode* node) {\n        if (node == nullptr) return;\n\n        inorderTraversal(node->left);\n        std::cout << node->data << \" \";\n        inorderTraversal(node->right);\n    }\n\n    // Helper method to delete the tree\n    void deleteTree(BSTNode* node) {\n        if (node == nullptr) return;\n\n        deleteTree(node->left);\n        deleteTree(node->right);\n        delete node;\n    }\n\npublic:\n    // Constructor\n    BinarySearchTree() : root(nullptr) {}\n\n    // Destructor\n    ~BinarySearchTree() {\n        deleteTree(root);\n    }\n\n    // Insert a value\n    void insert(int value) {\n        root = insertRecursive(root, value);\n    }\n\n    // Search for a value\n    bool search(int value) {\n        return searchRecursive(root, value) != nullptr;\n    }\n\n    // Delete a value\n    void remove(int value) {\n        root = deleteRecursive(root, value);\n    }\n\n    // Display the tree in-order\n    void inorder() {\n        std::cout << \"Inorder Traversal: \";\n        inorderTraversal(root);\n        std::cout << std::endl;\n    }\n};\n\nWith STL\n#include <iostream>\n#include <set>\n#include <vector>\n\nclass BSTWithSTL {\nprivate:\n    std::set<int> bst;\n\npublic:\n    // Insert a value\n    void insert(int value) {\n        bst.insert(value);\n    }\n\n    // Search for a value\n    bool search(int value) {\n        return bst.find(value) != bst.end();\n    }\n\n    // Delete a value\n    void remove(int value) {\n        bst.erase(value);\n    }\n\n    // Display the set (equivalent to in-order traversal)\n    void inorder() {\n        std::cout << \"Inorder Traversal: \";\n        for (const auto& val : bst) {\n            std::cout << val << \" \";\n        }\n        std::cout << std::endl;\n    }\n};\n\nCode Explanation\nWithout STL:\nWith STL:\nExample Input/Output\nint main() {\n    BinarySearchTree bst;\n\n    // Insert values\n    bst.insert(50);\n    bst.insert(30);\n    bst.insert(70);\n    bst.insert(20);\n    bst.insert(40);\n    bst.insert(60);\n    bst.insert(80);\n\n    // Display the tree\n    bst.inorder();  // Output: Inorder Traversal: 20 30 40 50 60 70 80\n\n    // Search operations\n    std::cout << \"Search 40: \" << (bst.search(40) ? \"Found\" : \"Not Found\") << std::endl;\n    std::cout << \"Search 90: \" << (bst.search(90) ? \"Found\" : \"Not Found\") << std::endl;\n\n    // Delete operations\n    bst.remove(20);  // Delete node with no child\n    bst.remove(30);  // Delete node with one child\n    bst.remove(50);  // Delete node with two children\n\n    // Display the modified tree\n    bst.inorder();  // Output: Inorder Traversal: 40 60 70 80\n\n    return 0;\n}\n\nComplexity Analysis\n[table block]\nNote: h is the height of the tree, which is O(log n) for balanced trees and O(n) for skewed trees.\n[divider block]\nAVL Tree\nIntroduction\nAn AVL tree (named after inventors Adelson-Velsky and Landis) is a self-balancing binary search tree where the height difference between left and right subtrees (balance factor) of any node cannot exceed 1. After any insertion or deletion, if the balance factor exceeds 1, rotations are performed to rebalance the tree.\nUse Cases & Applications\nDatabase indexing where frequent insertions and deletions occur\nMemory management systems\nHigh-performance dictionaries and maps\nApplications requiring guaranteed O(log n) operations\nAdvantages & Limitations\nAdvantages:\nGuaranteed O(log n) search, insert, and delete operations\nMore rigidly balanced than Red-Black trees\nEfficient for lookup-intensive applications\nLimitations:\nMore rotations needed than Red-Black trees (higher overhead for insertions/deletions)\nExtra space required for height/balance factor\nMore complex implementation than simple BST\nC++ Implementation\nWithout STL\n#include <iostream>\n#include <algorithm>\n\n// Node structure for AVL Tree\nstruct AVLNode {\n    int data;\n    int height;\n    AVLNode* left;\n    AVLNode* right;\n\n    // Constructor\n    AVLNode(int val) : data(val), height(1), left(nullptr), right(nullptr) {}\n};\n\nclass AVLTree {\nprivate:\n    AVLNode* root;\n\n    // Get height of a node (nullptr has height 0)\n    int height(AVLNode* node) {\n        return (node == nullptr) ? 0 : node->height;\n    }\n\n    // Get balance factor of a node\n    int getBalanceFactor(AVLNode* node) {\n        return (node == nullptr) ? 0 : height(node->left) - height(node->right);\n    }\n\n    // Update height of a node\n    void updateHeight(AVLNode* node) {\n        if (node != nullptr) {\n            node->height = 1 + std::max(height(node->left), height(node->right));\n        }\n    }\n\n    // Right rotation\n    AVLNode* rightRotate(AVLNode* y) {\n        AVLNode* x = y->left;\n        AVLNode* T2 = x->right;\n\n        // Perform rotation\n        x->right = y;\n        y->left = T2;\n\n        // Update heights\n        updateHeight(y);\n        updateHeight(x);\n\n        // Return new root\n        return x;\n    }\n\n    // Left rotation\n    AVLNode* leftRotate(AVLNode* x) {\n        AVLNode* y = x->right;\n        AVLNode* T2 = y->left;\n\n        // Perform rotation\n        y->left = x;\n        x->right = T2;\n\n        // Update heights\n        updateHeight(x);\n        updateHeight(y);\n\n        // Return new root\n        return y;\n    }\n\n    // Insert a node recursively\n    AVLNode* insertRecursive(AVLNode* node, int value) {\n        // 1. Perform standard BST insert\n        if (node == nullptr) {\n            return new AVLNode(value);\n        }\n\n        if (value < node->data) {\n            node->left = insertRecursive(node->left, value);\n        } else if (value > node->data) {\n            node->right = insertRecursive(node->right, value);\n        } else {\n            // Duplicate values not allowed\n            return node;\n        }\n\n        // 2. Update height of current node\n        updateHeight(node);\n\n        // 3. Get the balance factor\n        int balance = getBalanceFactor(node);\n\n        // 4. If unbalanced, handle 4 cases\n\n        // Left Left Case\n        if (balance > 1 && value < node->left->data) {\n            return rightRotate(node);\n        }\n\n        // Right Right Case\n        if (balance < -1 && value > node->right->data) {\n            return leftRotate(node);\n        }\n\n        // Left Right Case\n        if (balance > 1 && value > node->left->data) {\n            node->left = leftRotate(node->left);\n            return rightRotate(node);\n        }\n\n        // Right Left Case\n        if (balance < -1 && value < node->right->data) {\n            node->right = rightRotate(node->right);\n            return leftRotate(node);\n        }\n\n        // Return unchanged node pointer\n        return node;\n    }\n\n    // Find node with minimum value\n    AVLNode* findMinNode(AVLNode* node) {\n        AVLNode* current = node;\n        while (current && current->left != nullptr) {\n            current = current->left;\n        }\n        return current;\n    }\n\n    // Delete a node recursively\n    AVLNode* deleteRecursive(AVLNode* node, int value) {\n        // 1. Standard BST delete\n        if (node == nullptr) return nullptr;\n\n        if (value < node->data) {\n            node->left = deleteRecursive(node->left, value);\n        } else if (value > node->data) {\n            node->right = deleteRecursive(node->right, value);\n        } else {\n            // Node with only one child or no child\n            if (node->left == nullptr || node->right == nullptr) {\n                AVLNode* temp = node->left ? node->left : node->right;\n\n                // No child case\n                if (temp == nullptr) {\n                    temp = node;\n                    node = nullptr;\n                } else {\n                    // One child case\n                    *node = *temp; // Copy the contents\n                }\n\n                delete temp;\n            } else {\n                // Node with two children\n                AVLNode* temp = findMinNode(node->right);\n                node->data = temp->data;\n                node->right = deleteRecursive(node->right, temp->data);\n            }\n        }\n\n        // If the tree had only one node, return\n        if (node == nullptr) return nullptr;\n\n        // 2. Update height\n        updateHeight(node);\n\n        // 3. Get balance factor\n        int balance = getBalanceFactor(node);\n\n        // 4. If unbalanced, handle 4 cases\n\n        // Left Left Case\n        if (balance > 1 && getBalanceFactor(node->left) >= 0) {\n            return rightRotate(node);\n        }\n\n        // Left Right Case\n        if (balance > 1 && getBalanceFactor(node->left) < 0) {\n            node->left = leftRotate(node->left);\n            return rightRotate(node);\n        }\n\n        // Right Right Case\n        if (balance < -1 && getBalanceFactor(node->right) <= 0) {\n            return leftRotate(node);\n        }\n\n        // Right Left Case\n        if (balance < -1 && getBalanceFactor(node->right) > 0) {\n            node->right = rightRotate(node->right);\n            return leftRotate(node);\n        }\n\n        return node;\n    }\n\n    // Search for a value\n    AVLNode* searchRecursive(AVLNode* node, int value) {\n        if (node == nullptr || node->data == value) {\n            return node;\n        }\n\n        if (value < node->data) {\n            return searchRecursive(node->left, value);\n        }\n\n        return searchRecursive(node->right, value);\n    }\n\n    // Helper method for inorder traversal\n    void inorderTraversal(AVLNode* node) {\n        if (node == nullptr) return;\n\n        inorderTraversal(node->left);\n        std::cout << node->data << \"(\" << getBalanceFactor(node) << \") \";\n        inorderTraversal(node->right);\n    }\n\n    // Helper method to delete the tree\n    void deleteTree(AVLNode* node) {\n        if (node == nullptr) return;\n\n        deleteTree(node->left);\n        deleteTree(node->right);\n        delete node;\n    }\n\npublic:\n    // Constructor\n    AVLTree() : root(nullptr) {}\n\n    // Destructor\n    ~AVLTree() {\n        deleteTree(root);\n    }\n\n    // Insert a value\n    void insert(int value) {\n        root = insertRecursive(root, value);\n    }\n\n    // Search for a value\n    bool search(int value) {\n        return searchRecursive(root, value) != nullptr;\n    }\n\n    // Delete a value\n    void remove(int value) {\n        root = deleteRecursive(root, value);\n    }\n\n    // Display the tree in-order (with balance factors)\n    void inorder() {\n        std::cout << \"Inorder Traversal (with balance factors): \";\n        inorderTraversal(root);\n        std::cout << std::endl;\n    }\n};\n\nCode Explanation\nNode Structure:\nBalance Factor:\nRotations:\nInsertion and Deletion:\nExample Input/Output\nint main() {\n    AVLTree avl;\n\n    // Insert elements\n    avl.insert(10);\n    avl.insert(20);\n    avl.insert(30);  // This will cause rotations\n    avl.insert(40);\n    avl.insert(50);  // This will cause rotations\n    avl.insert(25);\n\n    // Display the tree\n    avl.inorder();\n\n    // Remove an element and display again\n    avl.remove(30);\n    avl.inorder();\n\n    return 0;\n}\n\n// Output:\n// Inorder Traversal (with balance factors): 10(-2) 20(0) 25(-1) 30(0) 40(1) 50(0)\n// Inorder Traversal (with balance factors): 10(-2) 20(0) 25(-1) 40(0) 50(0)\n\nComplexity Analysis\n[table block]\n[divider block]\nTree Data Structures\nTable of Contents\nRed-Black Tree\nB-Trees & B+ Trees\nHeap (Min Heap, Max Heap)\nTrie\nSegment Tree\nRed-Black Tree\nIntroduction\nA Red-Black Tree is a self-balancing binary search tree where each node has an extra bit representing \"color,\" which can be either red or black. This structure maintains balance through a set of properties that ensure the tree remains approximately balanced during insertions and deletions.\nProperties:\nEvery node is either red or black.\nThe root is black.\nEvery leaf (NIL/NULL) is black.\nIf a node is red, then both its children are black.\nAll simple paths from a node to descendant leaves contain the same number of black nodes.\nUse Cases & Applications\nImplementation of associative containers: Extensively used in C++ STL's std::map, std::multimap, std::set, and std::multiset.\nKernel data structures: Linux kernel's process scheduling, memory management, and filesystem implementations.\nDatabase indexing: Used in certain database management systems.\nNetworking: IP routing tables implementation.\nAdvantages & Limitations\nAdvantages:\nGuaranteed O(log n) time complexity for search, insert, and delete operations.\nMore efficient rebalancing compared to AVL trees (less rotations).\nExcellent for mixed operation workloads with frequent insertions and deletions.\nGood memory utilization compared to other self-balancing trees.\nLimitations:\nMore complex implementation than simpler trees.\nAdditional memory overhead for color information.\nNot as strictly balanced as AVL trees (height can be up to 2*log(n)).\nNot optimized for search-heavy workloads compared to AVL trees.\nC++ Implementation\nWithout STL\n#include <iostream>\n\nenum Color { RED, BLACK };\n\nstruct Node {\n    int data;\n    Color color;\n    Node *left, *right, *parent;\n\n    Node(int data) : data(data), color(RED), left(nullptr), right(nullptr), parent(nullptr) {}\n};\n\nclass RedBlackTree {\nprivate:\n    Node* root;\n    Node* NIL; // sentinel leaf node\n\n    // Helper functions\n    void leftRotate(Node* x);\n    void rightRotate(Node* y);\n    void insertFixup(Node* k);\n    void transplant(Node* u, Node* v);\n    void deleteFixup(Node* x);\n    Node* minimum(Node* node);\n    void inOrderHelper(Node* node);\n    void destroyNode(Node* node);\n\npublic:\n    RedBlackTree() {\n        NIL = new Node(0);\n        NIL->color = BLACK;\n        NIL->left = nullptr;\n        NIL->right = nullptr;\n        root = NIL;\n    }\n\n    ~RedBlackTree() {\n        destroyTree();\n        delete NIL;\n    }\n\n    void insert(int key);\n    void remove(int key);\n    bool search(int key);\n    void inOrder();\n    void destroyTree();\n};\n\nvoid RedBlackTree::leftRotate(Node* x) {\n    Node* y = x->right;    // Set y\n    x->right = y->left;    // Turn y's left subtree into x's right subtree\n\n    if (y->left != NIL)\n        y->left->parent = x;\n\n    y->parent = x->parent;  // Link x's parent to y\n\n    if (x->parent == nullptr)\n        root = y;\n    else if (x == x->parent->left)\n        x->parent->left = y;\n    else\n        x->parent->right = y;\n\n    y->left = x;            // Put x on y's left\n    x->parent = y;\n}\n\nvoid RedBlackTree::rightRotate(Node* y) {\n    Node* x = y->left;      // Set x\n    y->left = x->right;     // Turn x's right subtree into y's left subtree\n\n    if (x->right != NIL)\n        x->right->parent = y;\n\n    x->parent = y->parent;  // Link y's parent to x\n\n    if (y->parent == nullptr)\n        root = x;\n    else if (y == y->parent->right)\n        y->parent->right = x;\n    else\n        y->parent->left = x;\n\n    x->right = y;           // Put y on x's right\n    y->parent = x;\n}\n\nvoid RedBlackTree::insert(int key) {\n    Node* z = new Node(key);\n    z->left = NIL;\n    z->right = NIL;\n\n    Node* y = nullptr;\n    Node* x = root;\n\n    // Find the position to insert the new node\n    while (x != NIL) {\n        y = x;\n        if (z->data < x->data)\n            x = x->left;\n        else\n            x = x->right;\n    }\n\n    z->parent = y;\n    if (y == nullptr)\n        root = z;  // Tree was empty\n    else if (z->data < y->data)\n        y->left = z;\n    else\n        y->right = z;\n\n    // Fix the Red-Black properties that might have been violated\n    insertFixup(z);\n}\n\nvoid RedBlackTree::insertFixup(Node* k) {\n    Node* u;\n    while (k->parent != nullptr && k->parent->color == RED) {\n        if (k->parent == k->parent->parent->right) {\n            u = k->parent->parent->left;\n            if (u->color == RED) {\n                // Case 1: Uncle is red\n                u->color = BLACK;\n                k->parent->color = BLACK;\n                k->parent->parent->color = RED;\n                k = k->parent->parent;\n            } else {\n                if (k == k->parent->left) {\n                    // Case 2: Uncle is black and k is a left child\n                    k = k->parent;\n                    rightRotate(k);\n                }\n                // Case 3: Uncle is black and k is a right child\n                k->parent->color = BLACK;\n                k->parent->parent->color = RED;\n                leftRotate(k->parent->parent);\n            }\n        } else {\n            u = k->parent->parent->right;\n            if (u->color == RED) {\n                // Case 1: Uncle is red\n                u->color = BLACK;\n                k->parent->color = BLACK;\n                k->parent->parent->color = RED;\n                k = k->parent->parent;\n            } else {\n                if (k == k->parent->right) {\n                    // Case 2: Uncle is black and k is a right child\n                    k = k->parent;\n                    leftRotate(k);\n                }\n                // Case 3: Uncle is black and k is a left child\n                k->parent->color = BLACK;\n                k->parent->parent->color = RED;\n                rightRotate(k->parent->parent);\n            }\n        }\n        if (k == root) {\n            break;\n        }\n    }\n    root->color = BLACK;\n}\n\nbool RedBlackTree::search(int key) {\n    Node* current = root;\n    while (current != NIL) {\n        if (key == current->data)\n            return true;\n        else if (key < current->data)\n            current = current->left;\n        else\n            current = current->right;\n    }\n    return false;\n}\n\nNode* RedBlackTree::minimum(Node* node) {\n    while (node->left != NIL)\n        node = node->left;\n    return node;\n}\n\nvoid RedBlackTree::transplant(Node* u, Node* v) {\n    if (u->parent == nullptr)\n        root = v;\n    else if (u == u->parent->left)\n        u->parent->left = v;\n    else\n        u->parent->right = v;\n    v->parent = u->parent;\n}\n\nvoid RedBlackTree::remove(int key) {\n    Node* z = root;\n    while (z != NIL) {\n        if (z->data == key)\n            break;\n        else if (key < z->data)\n            z = z->left;\n        else\n            z = z->right;\n    }\n\n    if (z == NIL)\n        return;  // Key not found\n\n    Node* y = z;\n    Node* x;\n    Color y_original_color = y->color;\n\n    if (z->left == NIL) {\n        x = z->right;\n        transplant(z, z->right);\n    } else if (z->right == NIL) {\n        x = z->left;\n        transplant(z, z->left);\n    } else {\n        y = minimum(z->right);\n        y_original_color = y->color;\n        x = y->right;\n\n        if (y->parent == z) {\n            x->parent = y;\n        } else {\n            transplant(y, y->right);\n            y->right = z->right;\n            y->right->parent = y;\n        }\n\n        transplant(z, y);\n        y->left = z->left;\n        y->left->parent = y;\n        y->color = z->color;\n    }\n\n    if (y_original_color == BLACK)\n        deleteFixup(x);\n\n    delete z;\n}\n\nvoid RedBlackTree::deleteFixup(Node* x) {\n    while (x != root && x->color == BLACK) {\n        if (x == x->parent->left) {\n            Node* w = x->parent->right;\n            if (w->color == RED) {\n                // Case 1\n                w->color = BLACK;\n                x->parent->color = RED;\n                leftRotate(x->parent);\n                w = x->parent->right;\n            }\n            if (w->left->color == BLACK && w->right->color == BLACK) {\n                // Case 2\n                w->color = RED;\n                x = x->parent;\n            } else {\n                if (w->right->color == BLACK) {\n                    // Case 3\n                    w->left->color = BLACK;\n                    w->color = RED;\n                    rightRotate(w);\n                    w = x->parent->right;\n                }\n                // Case 4\n                w->color = x->parent->color;\n                x->parent->color = BLACK;\n                w->right->color = BLACK;\n                leftRotate(x->parent);\n                x = root;\n            }\n        } else {\n            Node* w = x->parent->left;\n            if (w->color == RED) {\n                // Case 1\n                w->color = BLACK;\n                x->parent->color = RED;\n                rightRotate(x->parent);\n                w = x->parent->left;\n            }\n            if (w->right->color == BLACK && w->left->color == BLACK) {\n                // Case 2\n                w->color = RED;\n                x = x->parent;\n            } else {\n                if (w->left->color == BLACK) {\n                    // Case 3\n                    w->right->color = BLACK;\n                    w->color = RED;\n                    leftRotate(w);\n                    w = x->parent->left;\n                }\n                // Case 4\n                w->color = x->parent->color;\n                x->parent->color = BLACK;\n                w->left->color = BLACK;\n                rightRotate(x->parent);\n                x = root;\n            }\n        }\n    }\n    x->color = BLACK;\n}\n\nvoid RedBlackTree::inOrder() {\n    inOrderHelper(root);\n    std::cout << std::endl;\n}\n\nvoid RedBlackTree::inOrderHelper(Node* node) {\n    if (node != NIL) {\n        inOrderHelper(node->left);\n        std::cout << node->data << \"(\" << (node->color == RED ? \"R\" : \"B\") << \") \";\n        inOrderHelper(node->right);\n    }\n}\n\nvoid RedBlackTree::destroyTree() {\n    destroyNode(root);\n    root = NIL;\n}\n\nvoid RedBlackTree::destroyNode(Node* node) {\n    if (node != NIL) {\n        destroyNode(node->left);\n        destroyNode(node->right);\n        delete node;\n    }\n}\n\nWith STL\n#include <iostream>\n#include <map>\n#include <set>\n\n// Using std::map which is typically implemented as a Red-Black Tree\nint main() {\n    // Using Red-Black Tree via std::map\n    std::map<int, std::string> rbMap;\n\n    // Insert operations\n    rbMap[10] = \"Ten\";\n    rbMap[5] = \"Five\";\n    rbMap[15] = \"Fifteen\";\n    rbMap[3] = \"Three\";\n    rbMap[7] = \"Seven\";\n\n    // Search operation\n    if (rbMap.find(5) != rbMap.end()) {\n        std::cout << \"Found: \" << rbMap[5] << std::endl;\n    }\n\n    // Iteration (in-order traversal)\n    std::cout << \"Map contents:\" << std::endl;\n    for (const auto& pair : rbMap) {\n        std::cout << pair.first << \": \" << pair.second << std::endl;\n    }\n\n    // Deletion\n    rbMap.erase(5);\n\n    // Similarly with std::set\n    std::set<int> rbSet = {10, 5, 15, 3, 7};\n\n    // Check if element exists\n    if (rbSet.count(5) > 0) {\n        std::cout << \"5 exists in the set\" << std::endl;\n    }\n\n    // Iterate through set\n    std::cout << \"Set contents:\" << std::endl;\n    for (int value : rbSet) {\n        std::cout << value << \" \";\n    }\n    std::cout << std::endl;\n\n    return 0;\n}\n\nCode Explanation\nManual Implementation:\nThe Red-Black Tree implementation uses a Node structure with color information and standard BST properties.\nleftRotate and rightRotate functions perform tree rebalancing.\ninsertFixup ensures the Red-Black properties are maintained after insertion through:\ndeleteFixup maintains the properties after deletion, handling various cases:\nAn NIL sentinel node is used to represent leaf nodes.\nSTL Implementation:\nLeverages C++ standard library's std::map and std::set containers that use Red-Black Trees internally.\nProvides the same guarantees as manual implementation without the implementation complexity.\nExample Input/Output\nManual Implementation:\n// Creating a Red-Black Tree\nRedBlackTree rbt;\nrbt.insert(10);\nrbt.insert(20);\nrbt.insert(5);\nrbt.insert(6);\nrbt.insert(12);\n\n// Output of inOrder traversal\n5(B) 6(R) 10(B) 12(R) 20(B)\n\n// After removing 10\n5(B) 6(B) 12(B) 20(B)\n\nSTL Implementation:\n// Input: Insert values into map\nrbMap[10] = \"Ten\";\nrbMap[5] = \"Five\";\nrbMap[15] = \"Fifteen\";\n\n// Output: Map contents\n5: Five\n10: Ten\n15: Fifteen\n\n// Check for value\nFound: Five\n\n// After removing 5\n10: Ten\n15: Fifteen\n\nComplexity Analysis\n[table block]\nThe balancing guarantees that the height of the tree remains O(log n), ensuring all operations have logarithmic time complexity.\nB-Trees & B+ Trees\nIntroduction\nB-Tree:\nA B-tree is a self-balancing tree data structure that maintains sorted data and allows searches, sequential access, insertions, and deletions in logarithmic time. Unlike binary trees, a B-tree node can have more than two children. A B-tree of order m has the following properties:\nEvery node has at most m children.\nEvery non-leaf node (except root) has at least \u2308m/2\u2309 children.\nThe root has at least 2 children if it is not a leaf node.\nAll leaf nodes appear on the same level.\nA non-leaf node with k children contains k-1 keys.\nB+ Tree:\nA B+ tree is a variation of a B-tree that stores all data in its leaf nodes while keeping an index structure in the inner nodes. Key differences from B-tree:\nData is stored only in leaf nodes; internal nodes only contain keys for guidance.\nLeaf nodes are linked together in a linked list, allowing for efficient range queries.\nAll keys present in internal nodes are also found in leaf nodes.\nUse Cases & Applications\nB-Tree:\nDatabase indexing: Widely used for primary indices in database systems.\nFile systems: Used in various file systems like NTFS, HFS+, and ext4.\nIn-memory dictionaries: When requiring efficient range operations.\nB+ Tree:\nDatabase systems: The most common implementation for indices in modern DBMS like MySQL, PostgreSQL.\nFile systems: Used in many modern filesystems, especially for directory structures.\nRange queries: Particularly efficient due to leaf node linking.\nSequential scans: Optimal for situations requiring sequential access.\nAdvantages & Limitations\nB-Tree Advantages:\nExcellent for disk-based or other secondary-storage applications.\nMaintains balance automatically, ensuring optimal search performance.\nReduces disk I/O operations due to high fanout.\nEnsures logarithmic height, even for large datasets.\nB-Tree Limitations:\nMore complex implementation than binary trees.\nMemory overhead for partially filled nodes.\nNot optimized for in-memory operations compared to some other structures.\nData retrieval can require multiple node accesses.\nB+ Tree Advantages:\nSequential access is more efficient than B-trees.\nRange queries are optimized due to leaf node linking.\nData retrieval is more efficient because all actual records are in leaves.\nLess node splitting operations than B-trees in some cases.\nB+ Tree Limitations:\nSlightly more complex than B-trees.\nCan use more space due to key duplication (keys in internal nodes and leaf nodes)."
  },
  {
    "id": "1cbf0b00-3427-8065-9657-dc2dd297cffd",
    "title": "PyTorch: From Beginner to Advanced",
    "content": "https://huyenchip.com/mlops/#ml_engineering_fundamentals\n1. PyTorch Basics\n1.1 Installation and Setup\nFirst, let's install PyTorch:\n# For CPU only\npip install torch torchvision\n\n# For GPU with CUDA 12.1\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n\n1.2 Importing PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n1.3 Basic Operations\n# Check PyTorch version\nprint(torch.__version__)\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n2. Tensors and Operations\n2.1 Creating Tensors\n# From Python lists/arrays\nx = torch.tensor([1, 2, 3, 4])\nprint(x)\n\n# Zeros and ones\nzeros = torch.zeros(2, 3)  # 2x3 tensor of zeros\nones = torch.ones(2, 3)    # 2x3 tensor of ones\nprint(zeros)\nprint(ones)\n\n# Random tensors\nrandom_tensor = torch.rand(2, 3)  # 2x3 tensor with random values from uniform distribution [0, 1)\nprint(random_tensor)\n\n# With specific data type\nint_tensor = torch.tensor([1, 2, 3], dtype=torch.int64)\nfloat_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\nprint(int_tensor.dtype)\nprint(float_tensor.dtype)\n\n# Creating tensors with specific device\ncpu_tensor = torch.tensor([1, 2, 3], device='cpu')\n# GPU tensor (if available)\nif torch.cuda.is_available():\n    gpu_tensor = torch.tensor([1, 2, 3], device='cuda')\n\n2.2 Tensor Properties and Operations\n# Tensor properties\nx = torch.rand(3, 4, 5)\nprint(f\"Shape: {x.shape}\")\nprint(f\"Dimensions: {x.dim()}\")\nprint(f\"Number of elements: {x.numel()}\")\nprint(f\"Data type: {x.dtype}\")\nprint(f\"Device: {x.device}\")\n\n# Indexing and slicing\nx = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(x[0])       # First row\nprint(x[:, 1])    # Second column\nprint(x[0:2, 1:]) # First two rows, from second column onwards\n\n# Basic operations\na = torch.tensor([1, 2, 3])\nb = torch.tensor([4, 5, 6])\n\n# Element-wise addition\nprint(a + b)\nprint(torch.add(a, b))\n\n# Element-wise multiplication\nprint(a * b)\nprint(torch.mul(a, b))\n\n# Matrix multiplication\nc = torch.tensor([[1, 2], [3, 4]])\nd = torch.tensor([[5, 6], [7, 8]])\nprint(torch.matmul(c, d))\nprint(c @ d)  # @ operator for matrix multiplication\n\n# In-place operations (saving memory)\na.add_(b)  # Equivalent to a = a + b\nprint(a)\n\n2.3 Reshaping Tensors\nx = torch.tensor([[1, 2, 3], [4, 5, 6]])\nprint(x.shape)  # torch.Size([2, 3])\n\n# Reshape\ny = x.reshape(3, 2)\nprint(y)\nprint(y.shape)  # torch.Size([3, 2])\n\n# View (shares the same data)\nz = x.view(6, 1)\nprint(z)\nprint(z.shape)  # torch.Size([6, 1])\n\n# Transpose\ntransposed = x.t()\nprint(transposed)\nprint(transposed.shape)  # torch.Size([3, 2])\n\n# Flatten\nflattened = x.flatten()\nprint(flattened)\nprint(flattened.shape)  # torch.Size([6])\n\n# Squeeze and unsqueeze\na = torch.zeros(2, 1, 3)\nprint(a.shape)  # torch.Size([2, 1, 3])\nsqueezed = a.squeeze(1)  # Remove dimension 1 (size 1)\nprint(squeezed.shape)  # torch.Size([2, 3])\n\nb = torch.zeros(2, 3)\nprint(b.shape)  # torch.Size([2, 3])\nunsqueezed = b.unsqueeze(1)  # Add dimension of size 1 at position 1\nprint(unsqueezed.shape)  # torch.Size([2, 1, 3])\n\n2.4 Advanced Tensor Operations\n# Concatenation\na = torch.tensor([[1, 2], [3, 4]])\nb = torch.tensor([[5, 6], [7, 8]])\ncat_dim0 = torch.cat((a, b), dim=0)  # Concatenate along rows\nprint(cat_dim0)\ncat_dim1 = torch.cat((a, b), dim=1)  # Concatenate along columns\nprint(cat_dim1)\n\n# Stacking\nstack = torch.stack((a, b), dim=0)  # Stack tensors along a new dimension\nprint(stack)\nprint(stack.shape)  # torch.Size([2, 2, 2])\n\n# Splitting\nc = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\nchunks = torch.chunk(c, chunks=2, dim=1)  # Split into 2 chunks along columns\nfor chunk in chunks:\n    print(chunk)\n\n# Statistical operations\nx = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\nprint(f\"Mean: {x.mean()}\")\nprint(f\"Sum: {x.sum()}\")\nprint(f\"Max: {x.max()}\")\nprint(f\"Min: {x.min()}\")\nprint(f\"Standard deviation: {x.std()}\")\nprint(f\"Column-wise mean: {x.mean(dim=0)}\")\nprint(f\"Row-wise sum: {x.sum(dim=1)}\")\n\n3. Autograd: Automatic Differentiation\n3.1 Tracking Operations with Autograd\n# Creating tensors with gradient tracking\nx = torch.tensor(2.0, requires_grad=True)\ny = torch.tensor(3.0, requires_grad=True)\n\n# Perform operations\nz = x**2 + y**3\n\n# Compute gradients\nz.backward()\n\n# Access gradients\nprint(f\"Gradient of x: {x.grad}\")  # dz/dx = 2x = 2*2 = 4\nprint(f\"Gradient of y: {y.grad}\")  # dz/dy = 3y^2 = 3*3^2 = 27\n\n3.2 Gradient Computation with Vector Outputs\n# Vector function example\nx = torch.tensor([2.0, 3.0], requires_grad=True)\ny = x**2\nz = y.sum()\n\n# Compute gradients\nz.backward()\nprint(f\"Gradient of x: {x.grad}\")  # [4.0, 6.0]\n\n# Reset gradients\nx.grad.zero_()\n\n# If we don't sum to a scalar\ny = x**2\n# We need to provide a gradient argument for non-scalar outputs\nexternal_grad = torch.tensor([1.0, 1.0])\ny.backward(gradient=external_grad)\nprint(f\"Gradient of x: {x.grad}\")  # [4.0, 6.0]\n\n3.3 Stopping Gradient Tracking\nx = torch.tensor(2.0, requires_grad=True)\ny = torch.tensor(3.0, requires_grad=True)\n\n# Method 1: with torch.no_grad()\nwith torch.no_grad():\n    z = x**2 + y**3\n    print(f\"Requires grad: {z.requires_grad}\")  # False\n\n# Method 2: using detach()\nz = (x**2 + y**3).detach()\nprint(f\"Requires grad: {z.requires_grad}\")  # False\n\n# Useful for model evaluation\ndef eval_mode_example(model, data):\n    model.eval()  # Set to evaluation mode\n    with torch.no_grad():\n        output = model(data)  # Forward pass without gradient tracking\n    return output\n\n4. Neural Network Fundamentals\n4.1 Building a Neural Network with nn.Module\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleNN, self).__init__()\n        # Layers\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # Forward pass\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Create an instance of the model\ninput_size = 10\nhidden_size = 20\noutput_size = 2\nmodel = SimpleNN(input_size, hidden_size, output_size)\n\n# Print model structure\nprint(model)\n\n# Generate random input\nbatch_size = 5\nx = torch.randn(batch_size, input_size)\n\n# Forward pass\noutput = model(x)\nprint(output.shape)  # torch.Size([5, 2])\n\n4.2 Common Layer Types\n# Sequential model with various layer types\nmodel = nn.Sequential(\n    nn.Linear(784, 256),                # Fully connected layer\n    nn.ReLU(),                          # Activation function\n    nn.BatchNorm1d(256),                # Batch normalization\n    nn.Dropout(0.2),                    # Dropout for regularization\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    nn.Linear(128, 10)\n)\n\n# Convolutional layers\nconv_model = nn.Sequential(\n    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(2, 2),\n    nn.Flatten(),\n    nn.Linear(32 * 8 * 8, 10)  # Assuming 32x32 input image\n)\n\n# Recurrent layers\nrnn_model = nn.Sequential(\n    nn.Embedding(num_embeddings=10000, embedding_dim=100),  # Word embeddings\n    nn.LSTM(input_size=100, hidden_size=128, num_layers=2, batch_first=True),\n    # Note: LSTM output needs special handling\n)\n\n# You would need to handle LSTM output differently:\nclass LSTMModel(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n        super(LSTMModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # x shape: (batch_size, sequence_length)\n        embedded = self.embedding(x)  # (batch_size, sequence_length, embed_size)\n\n        # LSTM returns: output, (hidden_state, cell_state)\n        lstm_out, _ = self.lstm(embedded)  # lstm_out: (batch_size, sequence_length, hidden_size)\n\n        # Using the last time step output for classification\n        out = self.fc(lstm_out[:, -1, :])  # (batch_size, output_size)\n        return out\n\n4.3 Loss Functions\nimport torch.nn as nn\n\n# Common loss functions\n# Binary Cross Entropy (for binary classification)\nbce_loss = nn.BCELoss()\nprediction = torch.tensor([0.7, 0.3, 0.9], dtype=torch.float32)\ntarget = torch.tensor([1.0, 0.0, 1.0], dtype=torch.float32)\nloss = bce_loss(prediction, target)\nprint(f\"Binary Cross Entropy Loss: {loss.item()}\")\n\n# Cross Entropy (for multi-class classification)\nce_loss = nn.CrossEntropyLoss()\nlogits = torch.tensor([[0.1, 0.2, 0.7], [0.3, 0.5, 0.2]], dtype=torch.float32)\ntargets = torch.tensor([2, 1], dtype=torch.long)  # Class indices\nloss = ce_loss(logits, targets)\nprint(f\"Cross Entropy Loss: {loss.item()}\")\n\n# Mean Squared Error (for regression)\nmse_loss = nn.MSELoss()\npredictions = torch.tensor([0.5, 0.3, 0.9], dtype=torch.float32)\ntargets = torch.tensor([0.4, 0.5, 0.8], dtype=torch.float32)\nloss = mse_loss(predictions, targets)\nprint(f\"MSE Loss: {loss.item()}\")\n\n# L1 Loss (Mean Absolute Error)\nl1_loss = nn.L1Loss()\nloss = l1_loss(predictions, targets)\nprint(f\"L1 Loss: {loss.item()}\")\n\n# Custom loss function\nclass CustomLoss(nn.Module):\n    def __init__(self, weight=1.0):\n        super(CustomLoss, self).__init__()\n        self.weight = weight\n\n    def forward(self, predictions, targets):\n        # Custom loss calculation\n        loss = self.weight * torch.mean((predictions - targets)**2)\n        return loss\n\ncustom_loss = CustomLoss(weight=2.0)\nloss = custom_loss(predictions, targets)\nprint(f\"Custom Loss: {loss.item()}\")\n\n4.4 Optimizers\nimport torch.optim as optim\n\n# Setup a simple model\nmodel = nn.Linear(10, 1)\n\n# SGD (Stochastic Gradient Descent)\nsgd_optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# Adam optimizer\nadam_optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n\n# RMSprop optimizer\nrmsprop_optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99)\n\n# AdamW (Adam with weight decay fix)\nadamw_optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n\n# Optimizer with learning rate scheduling\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n# Step scheduler: reduces learning rate by gamma every step_size epochs\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Example training loop with scheduler\nfor epoch in range(30):\n    # Training code here\n\n    # Update learning rate\n    scheduler.step()\n    print(f\"Epoch {epoch}, LR: {scheduler.get_last_lr()[0]}\")\n\n5. Training Neural Networks\n5.1 Basic Training Loop\ndef train(model, train_loader, criterion, optimizer, device, epochs=5):\n    model.train()  # Set model to training mode\n\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for i, (inputs, labels) in enumerate(train_loader):\n            # Move data to device (CPU/GPU)\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n\n            # Statistics\n            running_loss += loss.item()\n\n            if i % 100 == 99:  # Print every 100 mini-batches\n                print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], '\n                      f'Loss: {running_loss/100:.4f}')\n                running_loss = 0.0\n\n    print('Training completed')\n\n5.2 Evaluation Loop\ndef evaluate(model, test_loader, criterion, device):\n    model.eval()  # Set model to evaluation mode\n\n    total_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():  # Disable gradient computation\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Statistics\n            total_loss += loss.item()\n\n            # For classification\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    avg_loss = total_loss / len(test_loader)\n    accuracy = 100 * correct / total\n\n    print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n    return avg_loss, accuracy\n\n5.3 Complete Training Example with MNIST\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hyperparameters\nbatch_size = 64\nlearning_rate = 0.001\nnum_epochs = 5\n\n# Data transforms\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n])\n\n# Load datasets\ntrain_dataset = torchvision.datasets.MNIST(\n    root='./data', train=True, transform=transform, download=True)\ntest_dataset = torchvision.datasets.MNIST(\n    root='./data', train=False, transform=transform, download=True)\n\n# Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define the CNN model\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128, 10)\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        # Input: (batch_size, 1, 28, 28)\n        x = F.relu(self.conv1(x))  # (batch_size, 32, 28, 28)\n        x = self.pool(x)  # (batch_size, 32, 14, 14)\n\n        x = F.relu(self.conv2(x))  # (batch_size, 64, 14, 14)\n        x = self.pool(x)  # (batch_size, 64, 7, 7)\n\n        x = x.reshape(x.size(0), -1)  # (batch_size, 64*7*7)\n        x = F.relu(self.fc1(x))  # (batch_size, 128)\n        x = self.dropout(x)\n        x = self.fc2(x)  # (batch_size, 10)\n\n        return x\n\n# Initialize model, loss function and optimizer\nmodel = CNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\ntotal_steps = len(train_loader)\nfor epoch in range(num_epochs):\n    model.train()\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (i+1) % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')\n\n    # Evaluation\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in test_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {100 * correct / total:.2f}%')\n\n# Save the model\ntorch.save(model.state_dict(), 'mnist_cnn.pth')\n\n5.4 Custom Dataset and DataLoader\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport os\n\nclass CustomDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        self.data_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data_frame)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, 0])\n        image = Image.open(img_name).convert('RGB')\n        label = self.data_frame.iloc[idx, 1]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Example usage\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# dataset = CustomDataset(csv_file='annotations.csv', root_dir='images/', transform=transform)\n# dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\n\n# Example iterable dataset\nclass IterableCustomDataset(torch.utils.data.IterableDataset):\n    def __init__(self, start, end):\n        super(IterableCustomDataset).__init__()\n        self.start = start\n        self.end = end\n\n    def __iter__(self):\n        for i in range(self.start, self.end):\n            yield i, i * i\n\n# iterable_dataset = IterableCustomDataset(start=0, end=10)\n# for x, y in iterable_dataset:\n#     print(x, y)\n\n6. Computer Vision with PyTorch\n6.1 Working with Images and Torchvision\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import CIFAR10\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Data augmentation and normalization\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n])\n\n# Load CIFAR10 dataset\ntrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntest_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n\n# Function to show an image\ndef imshow(img):\n    img = img / 2 + 0.5  # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# Get some random training images\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\n# Show images\n# imshow(torchvision.utils.make_grid(images[:4]))\n\n6.2 Transfer Learning with Pre-trained Models\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import models\n\n# Preprocessing for pretrained models\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Example: Loading a pretrained ResNet and fine-tuning for a new task\ndef get_pretrained_model(num_classes):\n    # Load pre-trained ResNet-18\n    model = models.resnet18(pretrained=True)\n\n    # Freeze all layers\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Replace the final fully connected layer\n    # Parameters of newly constructed modules have requires_grad=True by default\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(num_ftrs, num_classes)\n\n    return model\n\n# Create model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = get_pretrained_model(num_classes=10).to(device)\n\n# Training with a pretrained model\ncriterion = nn.CrossEntropyLoss()\n\n# Only optimize the classifier parameters instead of all parameters\noptimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n\n# Later, if you want to fine-tune the entire model:\n# for param in model.parameters():\n#     param.requires_grad = True\n# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n6.3 Building a Convolutional Neural Network (CNN)\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(CNN, self).__init__()\n        # First convolutional layer\n        # input: (batch_size, 3, 32, 32), output: (batch_size, 16, 32, 32)\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(16)\n\n        # Second convolutional layer\n        # input: (batch_size, 16, 16, 16), output: (batch_size, 32, 16, 16)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(32)\n\n        # Third convolutional layer\n        # input: (batch_size, 32, 8, 8), output: (batch_size, 64, 8, 8)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(64)\n\n        # Pooling layer\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Fully connected layers\n        # input: (batch_size, 64 * 4 * 4), output: (batch_size, 512)\n        self.fc1 = nn.Linear(64 * 4 * 4, 512)\n        # input: (batch_size, 512), output: (batch_size, num_classes)\n        self.fc2 = nn.Linear(512, num_classes)\n\n        # Dropout layer for regularization\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        # First block\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # (batch_size, 16, 16, 16)\n\n        # Second block\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # (batch_size, 32, 8, 8)\n\n        # Third block\n        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # (batch_size, 64, 4, 4)\n\n        # Flatten\n        x = x\n"
  },
  {
    "id": "1ccf0b00-3427-80f7-a888-e640b3a7720e",
    "title": "Array Questions ",
    "content": "Question : Move zeros at the end \ninput=[1,2,0,3,4,0,0,1]\noutput =[1,2,3,4,1,0,0,0]\n// Move zeros at the end while not hurting the sequece of non zero numbers\n#include <iostream>\n#include <vector>\nusing namespace std;\nint main()\n{\n    vector<int> arr = {1, 0, 2, 3, 0, 0, 4, 0, 1};\n    int n = arr.size();\n    int j = -1;\n    for (int i = 0; i < n; i++)\n    {\n        if (arr[i] == 0)\n        {\n            j = i;\n            break;\n        }\n    }\n    cout << \"the value of j is \" << j;\n    if (j == -1)\n    {\n        cout << \"NO zero elements in the array \" << endl;\n\n        for (int i = 0; i < n; i++)\n        {\n            cout << arr[i] << \" \";\n        }\n        cout << endl;\n        return 0;\n    }\n    for (int k = j + 1; k < n; k++)\n    {\n        if (arr[k] != 0)\n        {\n            swap(arr[k], arr[j]);\n            j++;\n        }\n        }\n    // after the procedure is done\n    cout << \"after moving zeros \" << endl;\n    for (int i = 0; i < n; i++)\n    {\n        cout << arr[i] << \" \";\n    }\n    cout << endl;\n\n    return 0;\n}\nI want to make an interesting RAG application , I am currently studying extensively for my placements and I want to  timely revise  the topics I have read or done , I am building   my learning sources in NOTION  pages ,  I want to use NOTION API to access all the private notion pages I have for me like I have a 5 notion pages with different reading material okay , these material can be big ,\nSo I want to generate vector db  of any page   based on the timestamp of things I added in that notion from latest to old then select those  data , add that data in a file and then make a RAG ot of it , and using tat Rag simple , medium and hard questions to be generated by the llm that I have to answer and if failed their soluton comes to me . In this way I can automate my revision task . I want to deploy this websitr sing streamlit or vercel for free and access it\nFeel free to add more innovative ideas in this and tellme how can  I make such an application\n"
  },
  {
    "id": "135f0b00-3427-8028-9eff-fcf226dda7d0",
    "title": "DIGITAL FORENSICS SAMPLE QUESTIONS AND ANSWERS ",
    "content": "Digital Forensics: Incident Response and Data Acquisition - Exam Questions - Module 4\nEasy to Moderate Questions\nQuestion 1: Order of Volatility\nQuestion: Explain the concept of Order of Volatility in digital forensics. List the types of data from most volatile to least volatile.\nSolution:\nThe Order of Volatility is a principle in digital forensics that dictates the sequence of data collection during an investigation. The order from most to least volatile is:\nHighly Volatile Data: RAM, CPU registers\nModerately Volatile Data: Running processes, network connections\nLess Volatile Data: Hard drives, file systems, logs, removable media\nQuestion 2: Live vs. Dead Acquisition\nQuestion: Compare and contrast Live Acquisition and Dead Acquisition. Provide two specific use cases for each.\nSolution:\nLive Acquisition:\nInvolves collecting data from a running system\nCaptures real-time system state\nUse Cases:\nDead Acquisition:\nPerformed on a powered-off system\nMinimizes risk of data alteration\nUse Cases:\nQuestion 3: Write Blocking\nQuestion: What is write blocking, and why is it crucial in digital forensics?\nSolution:\nWrite blocking is a technique using tools or hardware that prevents any write operations to a storage device during evidence collection. It is crucial because it:\nEnsures the original evidence remains unaltered\nMaintains the integrity of digital evidence\nPreserves the admissibility of evidence in legal proceedings\nChallenging Questions\nQuestion 4: Data Acquisition Methodology Scenario\nScenario: You are a digital forensics investigator called to a corporate environment where a potential data breach is suspected. Outline the steps you would take to acquire digital evidence using the methodology discussed in the module.\nSolution:\nDetermine Acquisition Method:\nSelect Forensically Sound Tools:\nSanitize Target Media:\nAcquire Volatile Data:\nEnable Write Protection:\nAcquire Non-volatile Data:\nCreate Contingency Plan:\nValidate Acquisition:\nQuestion 5: Advanced Forensic Framework 4 (AFF4)\nQuestion: Describe the key innovations of AFF4 and how it addresses challenges in modern digital forensics.\nSolution:\nAFF4 Innovations:\nObject-oriented design with generic objects (volumes, streams)\nSupport for large-capacity storage media\nURL-based unique object addressing\nSupports multiple container formats (Zip, Zip64)\nIntroduces \"maps\" for efficient zero-copy data transformations\nSupports image signing and cryptography\nProvides image transparency\nUses globally unique identifiers\nQuestion 6: Forensic Data Acquisition Formats\nQuestion: Compare and contrast three different data acquisition formats, discussing their strengths and use cases.\nSolution:\nRaw/DD Format:\nE01 (EnCase Image):\nAFF (Advanced Forensic Format):\nQuestion 7: Comprehensive Forensic Investigation Scenario\nFull Solution Strategy:\nScenario: A multinational corporation suspects an insider threat involving data exfiltration. Design a comprehensive digital forensics strategy addressing legal, technical, and procedural challenges.\nPreliminary Preparation\nLegal Authorization\nObtain comprehensive legal clearance\nSecure search warrants for:\nInitial Risk Assessment\nConduct preliminary threat analysis\nIdentify potential data exfiltration vectors:\nTechnical Acquisition Strategy\nVolatile Data Collection\nImmediate live memory capture from:\nTools:\nNetwork Evidence Preservation\nCapture network traffic logs\nPreserve firewall and router configurations\nUse tools like:\nCollect:\nNon-Volatile Data Acquisition\nForensic imaging of:\nMethodologies:\nCryptographic Considerations\nPrepare for potential encrypted data\nTools for handling encryption:\nChain of Custody\nDocumentation Protocols\nDetailed logging of:\nUse standardized forensic documentation templates\nEvidence Preservation\nCreate multiple forensic copies\nUse write-blockers during acquisition\nStore evidence in secure, controlled environments\nMaintain hash integrity checks\nAnalysis Framework\nCorrelation of Evidence\nCross-reference:\nAdvanced Analysis Techniques\nTimeline reconstruction\nMetadata analysis\nCorrelation of disparate data sources\nAnomaly detection algorithms\nQuestion 8: Cryptographic Challenges in Forensic Acquisition\nDiscuss the technical and legal challenges of acquiring encrypted digital evidence and potential mitigation strategie\nComprehensive Solution:\nTechnical Encryption Challenges\nEncryption Types\nFull Disk Encryption (FDE)\nFile-level encryption\nCloud storage encryption\nEncrypted communication channels\nDecryption Approaches\nLegal compulsion methods\nCryptanalysis techniques\nKey recovery strategies\nComputational decryption approaches\nLegal Framework Considerations\nInternational Legal Challenges\nJurisdictional variations in encryption laws\nPrivacy protection regulations\nCross-border investigation complexities\nConsent and Compulsion\nLegal mechanisms for:\nForensic Tool Adaptations\nAdvanced Decryption Techniques\nComputational brute-force methods\nSide-channel attacks\nForensic key recovery tools\nMachine learning-assisted decryption\nEthical Considerations\nMinimal intrusion principle\nProportionality of decryption efforts\nPrivacy protection mechanisms\nQuestion 9: Emerging Technologies Impact on Digital Forensics\nAnalyze how cloud computing, IoT, and distributed systems challenge traditional digital forensics methodologies.\nComprehensive Analysis:\nCloud Computing Challenges\nEvidence Fragmentation\nDistributed data storage\nMulti-tenant infrastructure\nComplex access logging\nJurisdictional data sovereignty\nForensic Collection Strategies\nCloud provider forensic APIs\nSpecialized cloud forensic tools\nMetadata preservation techniques\nIoT Forensic Complexities\nDevice Diversity\nHeterogeneous device ecosystems\nLimited forensic capabilities\nVolatile and ephemeral data storage\nAcquisition Methodologies\nFirmware forensics\nNetwork traffic analysis\nEmbedded system investigation techniques\nDistributed Systems\nEvidence Correlation\nComplex interaction tracking\nTimestamp synchronization\nDistributed event reconstruction\nAdvanced Forensic Techniques\nMachine learning correlation\nBlockchain forensics\nDistributed ledger investigation\nQuestion 10: Ethical and Legal Considerations Framework\nDevelop a framework for ensuring ethical digital evidence collection that respects individual privacy while maintaining forensic integrity.\nComprehensive Ethical Framework:\nPrivacy Protection Principles\nMinimal Intrusion\nTargeted evidence collection\nProportional investigation scope\nAvoid unnecessary data exposure\nConsent Mechanisms\nExplicit informed consent\nClear communication of investigation parameters\nRight to legal representation\nLegal Compliance\nRegulatory Adherence\nGDPR compliance\nLocal data protection laws\nInternational privacy standards\nEvidence Admissibility\nForensically sound collection methods\nTransparent documentation\nVerifiable acquisition processes\nTechnological Safeguards\nData Anonymization\nPersonally identifiable information (PII) protection\nRedaction techniques\nSecure data handling protocols\nEthical Tool Development\nPrivacy-preserving forensic technologies\nTransparent algorithmic processes\nOngoing ethical review mechanisms\nModule 5 Questions \nQuestion 1: Advanced Windows Registry Forensics\nScenario: You are a digital forensics investigator examining a compromised Windows system. The suspect is accused of unauthorized access and potential data exfiltration.\nTask:\na) Explain the significance of the different registry hives and what critical information can be extracted from HKEY_USERS and HKEY_LOCAL_MACHINE.\nb) Describe how you would use registry analysis to reconstruct the timeline of user activities and system changes.\nSolution:\na) Registry Hives Analysis:\nHKEY_USERS (HKU):\nHKEY_LOCAL_MACHINE (HKLM):\nb) Timeline Reconstruction Methodology:\nExamine LastWrite times in registry keys to establish chronological events\nCross-reference user profile changes with system events\nAnalyze installed software, network configurations, and user account modifications\nLook for:\nQuestion 2: Advanced Memory Forensics with Volatility\nScenario: You are investigating a potential advanced persistent threat (APT) in a corporate network environment."
  }
]